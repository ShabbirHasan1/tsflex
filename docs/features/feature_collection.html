<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>tsflex.features.feature_collection API documentation</title>
<meta name="description" content="FeatureCollection class for bookkeeping and calculation of time-series features â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/foundation.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#ebf3ff}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#edfcf4}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'G-C88NHWRRP8', 'auto'); ga('send', 'pageview');
</script><script async src='https://www.google-analytics.com/analytics.js'></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="https://media.discordapp.net/attachments/372491075153166338/852906324417445908/icon.png">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tsflex.features.feature_collection</code></h1>
</header>
<section id="section-intro">
<p>FeatureCollection class for bookkeeping and calculation of time-series features.</p>
<h2 id="see-also">See Also</h2>
<p><code>Example notebooks and model serialization documentation.</code></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;FeatureCollection class for bookkeeping and calculation of time-series features.

See Also
--------
Example notebooks and model serialization documentation.

&#34;&#34;&#34;

from __future__ import annotations  # Make typing work for the enclosing class

__author__ = &#34;Jonas Van Der Donckt, Emiel Deprost, Jeroen Van Der Donckt&#34;

import os
import uuid
from copy import deepcopy
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import dill
import numpy as np
import pandas as pd
from pathos.multiprocessing import ProcessPool
from tqdm.auto import tqdm

from .feature import FeatureDescriptor, MultipleFeatureDescriptors
from .logger import logger
from .strided_rolling import StridedRolling
from ..features.function_wrapper import FuncWrapper
from ..utils.data import to_list, to_series_list, flatten
from ..utils.logging import delete_logging_handlers, add_logging_handler
from ..utils.time import timedelta_to_str


class FeatureCollection:
    &#34;&#34;&#34;Create a FeatureCollection.

    Parameters
    ----------
    feature_descriptors : Union[FeatureDescriptor, MultipleFeatureDescriptors, List[Union[FeatureDescriptor, MultipleFeatureDescriptors]]], optional
        Initial (list of) feature(s) to add to collection, by default None

    &#34;&#34;&#34;

    def __init__(
        self,
        feature_descriptors: Optional[
            Union[
                FeatureDescriptor,
                MultipleFeatureDescriptors,
                List[Union[FeatureDescriptor, MultipleFeatureDescriptors]],
            ]
        ] = None,
    ):
        # The feature collection is a dict with keys of type:
        #   tuple(tuple(str), float OR pd.timedelta, float OR pd.timedelta)
        # The outer tuple&#39;s values correspond to (series_key(s), window, stride)
        self._feature_desc_dict: Dict[
            Tuple[Tuple[str, ...], pd.Timedelta, pd.Timedelta], List[FeatureDescriptor]
        ] = {}

        if feature_descriptors:
            self.add(feature_descriptors)

    def get_required_series(self) -&gt; List[str]:
        &#34;&#34;&#34;Return all required series names for this feature collection.

        Return the list of series names that are required in order to calculate all the
        features (defined by the `FeatureDescriptor` objects) of this feature
        collection.

        Returns
        -------
        List[str]
            List of all the required series names.

        &#34;&#34;&#34;
        return list(
            set(flatten([fr_key[0] for fr_key in self._feature_desc_dict.keys()]))
        )

    @staticmethod
    def _get_collection_key(
        feature: FeatureDescriptor,
    ) -&gt; Tuple[tuple, pd.Timedelta, pd.Timedelta]:
        # Note: `window` &amp; `stride` properties can either be a pd.Timedelta or an int
        return feature.series_name, feature.window, feature.stride

    def _add_feature(self, feature: FeatureDescriptor):
        &#34;&#34;&#34;Add a `FeatureDescriptor` instance to the collection.

        Parameters
        ----------
        feature : FeatureDescriptor
            The feature that will be added to this feature collection.

        &#34;&#34;&#34;
        series_win_stride_key = self._get_collection_key(feature)
        if series_win_stride_key in self._feature_desc_dict.keys():
            added_output_names = flatten(
                f.function.output_names
                for f in self._feature_desc_dict[series_win_stride_key]
            )
            # Check that not a feature with the same output_name(s) is already added
            # for the series_win_stride_key
            assert not any(
                output_name in added_output_names
                for output_name in feature.function.output_names
            )
            self._feature_desc_dict[series_win_stride_key].append(feature)
        else:
            self._feature_desc_dict[series_win_stride_key] = [feature]

    def add(
        self,
        features: Union[
            FeatureDescriptor, 
            MultipleFeatureDescriptors,
            FeatureCollection,
            List[
                Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection]
            ],
        ],
    ):
        &#34;&#34;&#34;Add feature(s) to the FeatureCollection.

        Parameters
        ----------
        features : Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection, List[Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection]]]
            Feature(s) (containers) whose contained features will be added.

        Raises
        ------
        TypeError
            Raised when an item within `features` is not an instance of
            [`MultipleFeatureDescriptors`, `FeatureDescriptors`, `FeatureCollection`].

        &#34;&#34;&#34;
        # Convert to list if necessary
        features = to_list(features)

        for feature in features:
            if isinstance(feature, MultipleFeatureDescriptors):
                self.add(feature.feature_descriptions)
            elif isinstance(feature, FeatureDescriptor):
                self._add_feature(feature)
            elif isinstance(feature, FeatureCollection):
                # List needs to be flattened
                self.add(list(flatten(feature._feature_desc_dict.values())))
            else:
                raise TypeError(f&#34;type: {type(feature)} is not supported - {feature}&#34;)

    @staticmethod
    def _executor(idx: int):
        # global get_stroll_func
        stroll, function = get_stroll_func(idx)
        return stroll.apply_func(function)

    def _stroll_feat_generator(
        self, series_dict: Dict[str, pd.Series], window_idx: str, approve_sparsity: bool
    ) -&gt; List[Tuple[StridedRolling, FuncWrapper]]:
        # --- Future work ---
        # We could also make the StridedRolling creation multithreaded
        # Very low priority because the STROLL __init__ is rather efficient!
        keys_wins_strides = list(self._feature_desc_dict.keys())
        lengths = np.cumsum(
            [len(self._feature_desc_dict[k]) for k in keys_wins_strides]
        )

        def get_stroll_function(idx):
            key_idx = np.searchsorted(lengths, idx, &#34;right&#34;)  # right bc idx starts at 0
            key, win, stride = keys_wins_strides[key_idx]
            feature = self._feature_desc_dict[keys_wins_strides[key_idx]][
                idx - lengths[key_idx]
            ]
            function: FuncWrapper = feature.function
            stroll = StridedRolling(
                data=[series_dict[k] for k in key],
                window=win,
                stride=stride,
                window_idx=window_idx,
                approve_sparsity=approve_sparsity,
                data_type=function.input_type,
            )
            return stroll, function

        return get_stroll_function

    def _get_stroll_feat_length(self) -&gt; int:
        return sum(
            len(self._feature_desc_dict[k]) for k in self._feature_desc_dict.keys()
        )

    def calculate(
        self,
        data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],
        return_df: Optional[bool] = False,
        window_idx: Optional[str] = &#34;end&#34;,
        approve_sparsity: Optional[bool] = False,
        show_progress: Optional[bool] = False,
        logging_file_path: Optional[Union[str, Path]] = None,
        n_jobs: Optional[int] = None,
    ) -&gt; Union[List[pd.DataFrame], pd.DataFrame]:
        &#34;&#34;&#34;Calculate features on the passed data.

        Notes
        ------
        * The (column-)names of the series in `data` represent the names in the keys.
        * If a `logging_file_path` is provided, the execution (time) info can be
          retrieved by calling `logger.get_feature_logs(logging_file_path)`.
          Be aware that the `logging_file_path` gets cleared before the logger pushes
          logged messages. Hence, one should use a separate logging file for each
          constructed processing and feature instance with this library.

        Parameters
        ----------
        data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]
            Dataframe or Series or list thereof, with all the required data for the
            feature calculation. \n
            **Remark**: each Series / DataFrame must have a `pd.DatetimeIndex`.
            **Remark**: we assume that each name / column is unique.
        return_df : bool, optional
            Whether the output needs to be a dataframe list or a DataFrame, by default
            False.
            If `True` the output dataframes will be merged to a DataFrame with an outer
            merge.
        window_idx : str, optional
            The window&#39;s index position which will be used as index for the
            feature_window aggregation. Must be either of: [&#39;begin&#39;, &#39;middle&#39;, &#39;end&#39;],
            by default &#39;end&#39;. All features in this collection will use the same
            window_idx.
        approve_sparsity: bool, optional
            Bool indicating whether the user acknowledges that there may be sparsity
            (i.e., irregularly sampled data), by default False.
            If False and sparsity is observed, a warning is raised.
        show_progress: bool, optional
            If True, the progress will be shown with a progressbar, by default False.
        logging_file_path : Union[str, Path], optional
            The file path where the logged messages are stored. If `None`, then no
            logging `FileHandler` will be used and the logging messages are only pushed
            to stdout. Otherwise, a logging `FileHandler` will write the logged messages
            to the given file path.
        n_jobs : int, optional
            The number of processes used for the feature calculation. If `None`, then
            the number returned by `os.cpu_count()` is used, by default None. \n
            If n_jobs is either 0 or 1, the code will be executed sequentially without
            creating a process pool. This is very useful when debugging, as the stack
            trace will be more comprehensible.

            .. tip::
                * It takes on avg. _300ms_ to schedule everything with
                  multiprocessing. So if your feature extraction code runs faster than
                  ~1.5s, it might not be worth it to parallelize the process
                  (and thus better set the `n_jobs` to 0-1).
                * This method its memory peaks are significantly lower when executed
                  sequentially. Set the `n_jobs` to 0-1 if this matters.

        Returns
        -------
        Union[List[pd.DataFrame], pd.DataFrame]
            The calculated features.

        Raises
        ------
        KeyError
            Raised when a required key is not found in `data`.

        &#34;&#34;&#34;
        # Delete other logging handlers
        delete_logging_handlers(logger)
        # Add logging handler (if path provided)
        if logging_file_path:
            add_logging_handler(logger, logging_file_path)

        # Convert the data to a series_dict
        series_dict: Dict[str, pd.Series] = {}
        for s in to_series_list(data):
            # Assert the assumptions we make!
            assert isinstance(s.index, pd.DatetimeIndex)
            assert s.index.is_monotonic_increasing

            if s.name in self.get_required_series():
                series_dict[str(s.name)] = s

        # Note: this variable has a global scope so this is shared in multiprocessing
        global get_stroll_func
        get_stroll_func = self._stroll_feat_generator(
            series_dict, window_idx, approve_sparsity
        )
        nb_stroll_funcs = self._get_stroll_feat_length()

        if n_jobs is None:
            n_jobs = os.cpu_count()
        n_jobs = min(n_jobs, nb_stroll_funcs)

        if n_jobs in [0, 1]:
            idxs = range(nb_stroll_funcs)
            if show_progress:
                idxs = tqdm(idxs)
            calculated_feature_list = [self._executor(idx) for idx in idxs]
        else:
            # https://pathos.readthedocs.io/en/latest/pathos.html#usage
            with ProcessPool(nodes=n_jobs, source=True) as pool:
                results = pool.uimap(
                    self._executor,
                    range(nb_stroll_funcs)
                )
                if show_progress:
                    results = tqdm(results, total=self._get_stroll_feat_length())
                calculated_feature_list = [f for f in results]
                # Close &amp; join - see: https://github.com/uqfoundation/pathos/issues/131
                pool.close()
                pool.join()
                # Clear because: https://github.com/uqfoundation/pathos/issues/111
                pool.clear()

        if return_df:
            return pd.concat(calculated_feature_list, axis=1, join=&#34;outer&#34;, copy=False)
        else:
            return calculated_feature_list

    def serialize(self, file_path: Union[str, Path]):
        &#34;&#34;&#34;Serialize this `FeatureCollection` instance.

        Parameters
        ----------
        file_path : Union[str, Path]
            The path where the `FeatureCollection` will be serialized.

        Notes
        -----
        * As we use [Dill](https://github.com/uqfoundation/dill){:target=&#34;_blank&#34;} to
          serialize the files, we can **also serialize functions which are defined in
          the local scope, like lambdas.**

        &#34;&#34;&#34;
        with open(file_path, &#34;wb&#34;) as f:
            dill.dump(self, f, recurse=True)

    def reduce(self, feat_cols_to_keep: List[str]) -&gt; FeatureCollection:
        &#34;&#34;&#34;Create a reduced FeatureCollection instance based on feat_cols_to_keep.

        For example, this is useful to optimize feature-extraction inference
        (for your selected features) after performing a feature-selection procedure.

        Parameters
        ----------
        feat_cols_to_keep: List[str]
            A subset of the feature collection instance its column names.
            This corresponds to the columns / names of the output from `calculate`
            method that you want to keep.

        Returns
        -------
        FeatureCollection
            A new FeatureCollection object, which only withholds the FeatureDescriptors
            which constitute the `feat_cols_to_keep` output.

        Note
        -----
        Some FeatureDescriptor objects may have multiple **output-names**.&lt;br&gt;
        Hence, if you only want to retain _a subset_ of that FeatureDescriptor its
        feature outputs, you will still get **all features** as the new
        FeatureCollection is constructed by applying a filter on de FeatureDescriptor
        list and we thus not alter these FeatureDescriptor objects themselves.

        &#34;&#34;&#34;
        # dict in which we store all the { output_col_name : (UUID, FeatureDescriptor) }
        # items of our current Featurecollection object
        feat_col_fd_mapping: Dict[str, Tuple[str, FeatureDescriptor]] = {}
        for (s_names, window, stride), fds in self._feature_desc_dict.items():
            fd: FeatureDescriptor
            for fd in fds:
                # As a single FeatureDescriptor can have multiple output col names, we
                # create a unique identifier for each FeatureDescriptor (on which we
                # will apply set-like operations later on to only retain all the unique
                # FeatureDescriptors)
                uuid_str = str(uuid.uuid4())
                for output_name in fd.function.output_names:
                    # Reconstruct the feature column name
                    feat_col_name = &#39;__&#39;.join([
                        &#39;|&#39;.join(s_names) if isinstance(s_names, tuple) else s_names,
                        output_name,
                        f&#39;w={timedelta_to_str(window)}_s={timedelta_to_str(stride)}&#39;
                    ])
                    feat_col_fd_mapping[feat_col_name] = (uuid_str, fd)

        assert all(fc in feat_col_fd_mapping for fc in feat_cols_to_keep)

        # Collect (uuid, FeatureDescriptor) for the feat_cols_to_keep
        fd_subset: List[Tuple[str, FeatureDescriptor]] = [
            feat_col_fd_mapping[fc] for fc in feat_cols_to_keep
        ]

        # Reduce to unique feature descriptor objects (based on uuid) and create a new
        # FeatureCollection for their deepcopy&#39;s.
        seen_uuids = set()
        return FeatureCollection(
            feature_descriptors=[
                deepcopy(unique_fd) for unique_fd in
                {
                    fd for (uuid_str, fd) in fd_subset
                    if uuid_str not in seen_uuids and not seen_uuids.add(uuid_str)
                }
            ]
        )

    def __repr__(self) -&gt; str:
        &#34;&#34;&#34;Representation string of a FeatureCollection.&#34;&#34;&#34;
        feature_keys = sorted(set(k[0] for k in self._feature_desc_dict.keys()))
        output_str = &#34;&#34;
        for feature_key in feature_keys:
            output_str += f&#34;{&#39;|&#39;.join(feature_key)}: (&#34;
            keys = (x for x in self._feature_desc_dict.keys() if x[0] == feature_key)
            for _, win_size, stride in keys:
                output_str += f&#34;\n\twin: &#34;
                win_str = timedelta_to_str(win_size)
                stride_str = timedelta_to_str(stride)
                output_str += f&#34;{str(win_str):&lt;6}, stride: {str(stride_str)}: [&#34;
                for feat_desc in self._feature_desc_dict[feature_key, win_size, stride]:
                    output_str += f&#34;\n\t\t{feat_desc._func_str},&#34;
                output_str += &#34;\n\t]&#34;
            output_str += &#34;\n)\n&#34;
        return output_str</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tsflex.features.feature_collection.FeatureCollection"><code class="flex name class">
<span>class <span class="ident">FeatureCollection</span></span>
<span>(</span><span>feature_descriptors=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a FeatureCollection.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>feature_descriptors</code></strong> :&ensp;<code>Union[FeatureDescriptor, MultipleFeatureDescriptors, List[Union[FeatureDescriptor, MultipleFeatureDescriptors]]]</code>, optional</dt>
<dd>Initial (list of) feature(s) to add to collection, by default None</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureCollection:
    &#34;&#34;&#34;Create a FeatureCollection.

    Parameters
    ----------
    feature_descriptors : Union[FeatureDescriptor, MultipleFeatureDescriptors, List[Union[FeatureDescriptor, MultipleFeatureDescriptors]]], optional
        Initial (list of) feature(s) to add to collection, by default None

    &#34;&#34;&#34;

    def __init__(
        self,
        feature_descriptors: Optional[
            Union[
                FeatureDescriptor,
                MultipleFeatureDescriptors,
                List[Union[FeatureDescriptor, MultipleFeatureDescriptors]],
            ]
        ] = None,
    ):
        # The feature collection is a dict with keys of type:
        #   tuple(tuple(str), float OR pd.timedelta, float OR pd.timedelta)
        # The outer tuple&#39;s values correspond to (series_key(s), window, stride)
        self._feature_desc_dict: Dict[
            Tuple[Tuple[str, ...], pd.Timedelta, pd.Timedelta], List[FeatureDescriptor]
        ] = {}

        if feature_descriptors:
            self.add(feature_descriptors)

    def get_required_series(self) -&gt; List[str]:
        &#34;&#34;&#34;Return all required series names for this feature collection.

        Return the list of series names that are required in order to calculate all the
        features (defined by the `FeatureDescriptor` objects) of this feature
        collection.

        Returns
        -------
        List[str]
            List of all the required series names.

        &#34;&#34;&#34;
        return list(
            set(flatten([fr_key[0] for fr_key in self._feature_desc_dict.keys()]))
        )

    @staticmethod
    def _get_collection_key(
        feature: FeatureDescriptor,
    ) -&gt; Tuple[tuple, pd.Timedelta, pd.Timedelta]:
        # Note: `window` &amp; `stride` properties can either be a pd.Timedelta or an int
        return feature.series_name, feature.window, feature.stride

    def _add_feature(self, feature: FeatureDescriptor):
        &#34;&#34;&#34;Add a `FeatureDescriptor` instance to the collection.

        Parameters
        ----------
        feature : FeatureDescriptor
            The feature that will be added to this feature collection.

        &#34;&#34;&#34;
        series_win_stride_key = self._get_collection_key(feature)
        if series_win_stride_key in self._feature_desc_dict.keys():
            added_output_names = flatten(
                f.function.output_names
                for f in self._feature_desc_dict[series_win_stride_key]
            )
            # Check that not a feature with the same output_name(s) is already added
            # for the series_win_stride_key
            assert not any(
                output_name in added_output_names
                for output_name in feature.function.output_names
            )
            self._feature_desc_dict[series_win_stride_key].append(feature)
        else:
            self._feature_desc_dict[series_win_stride_key] = [feature]

    def add(
        self,
        features: Union[
            FeatureDescriptor, 
            MultipleFeatureDescriptors,
            FeatureCollection,
            List[
                Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection]
            ],
        ],
    ):
        &#34;&#34;&#34;Add feature(s) to the FeatureCollection.

        Parameters
        ----------
        features : Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection, List[Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection]]]
            Feature(s) (containers) whose contained features will be added.

        Raises
        ------
        TypeError
            Raised when an item within `features` is not an instance of
            [`MultipleFeatureDescriptors`, `FeatureDescriptors`, `FeatureCollection`].

        &#34;&#34;&#34;
        # Convert to list if necessary
        features = to_list(features)

        for feature in features:
            if isinstance(feature, MultipleFeatureDescriptors):
                self.add(feature.feature_descriptions)
            elif isinstance(feature, FeatureDescriptor):
                self._add_feature(feature)
            elif isinstance(feature, FeatureCollection):
                # List needs to be flattened
                self.add(list(flatten(feature._feature_desc_dict.values())))
            else:
                raise TypeError(f&#34;type: {type(feature)} is not supported - {feature}&#34;)

    @staticmethod
    def _executor(idx: int):
        # global get_stroll_func
        stroll, function = get_stroll_func(idx)
        return stroll.apply_func(function)

    def _stroll_feat_generator(
        self, series_dict: Dict[str, pd.Series], window_idx: str, approve_sparsity: bool
    ) -&gt; List[Tuple[StridedRolling, FuncWrapper]]:
        # --- Future work ---
        # We could also make the StridedRolling creation multithreaded
        # Very low priority because the STROLL __init__ is rather efficient!
        keys_wins_strides = list(self._feature_desc_dict.keys())
        lengths = np.cumsum(
            [len(self._feature_desc_dict[k]) for k in keys_wins_strides]
        )

        def get_stroll_function(idx):
            key_idx = np.searchsorted(lengths, idx, &#34;right&#34;)  # right bc idx starts at 0
            key, win, stride = keys_wins_strides[key_idx]
            feature = self._feature_desc_dict[keys_wins_strides[key_idx]][
                idx - lengths[key_idx]
            ]
            function: FuncWrapper = feature.function
            stroll = StridedRolling(
                data=[series_dict[k] for k in key],
                window=win,
                stride=stride,
                window_idx=window_idx,
                approve_sparsity=approve_sparsity,
                data_type=function.input_type,
            )
            return stroll, function

        return get_stroll_function

    def _get_stroll_feat_length(self) -&gt; int:
        return sum(
            len(self._feature_desc_dict[k]) for k in self._feature_desc_dict.keys()
        )

    def calculate(
        self,
        data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],
        return_df: Optional[bool] = False,
        window_idx: Optional[str] = &#34;end&#34;,
        approve_sparsity: Optional[bool] = False,
        show_progress: Optional[bool] = False,
        logging_file_path: Optional[Union[str, Path]] = None,
        n_jobs: Optional[int] = None,
    ) -&gt; Union[List[pd.DataFrame], pd.DataFrame]:
        &#34;&#34;&#34;Calculate features on the passed data.

        Notes
        ------
        * The (column-)names of the series in `data` represent the names in the keys.
        * If a `logging_file_path` is provided, the execution (time) info can be
          retrieved by calling `logger.get_feature_logs(logging_file_path)`.
          Be aware that the `logging_file_path` gets cleared before the logger pushes
          logged messages. Hence, one should use a separate logging file for each
          constructed processing and feature instance with this library.

        Parameters
        ----------
        data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]
            Dataframe or Series or list thereof, with all the required data for the
            feature calculation. \n
            **Remark**: each Series / DataFrame must have a `pd.DatetimeIndex`.
            **Remark**: we assume that each name / column is unique.
        return_df : bool, optional
            Whether the output needs to be a dataframe list or a DataFrame, by default
            False.
            If `True` the output dataframes will be merged to a DataFrame with an outer
            merge.
        window_idx : str, optional
            The window&#39;s index position which will be used as index for the
            feature_window aggregation. Must be either of: [&#39;begin&#39;, &#39;middle&#39;, &#39;end&#39;],
            by default &#39;end&#39;. All features in this collection will use the same
            window_idx.
        approve_sparsity: bool, optional
            Bool indicating whether the user acknowledges that there may be sparsity
            (i.e., irregularly sampled data), by default False.
            If False and sparsity is observed, a warning is raised.
        show_progress: bool, optional
            If True, the progress will be shown with a progressbar, by default False.
        logging_file_path : Union[str, Path], optional
            The file path where the logged messages are stored. If `None`, then no
            logging `FileHandler` will be used and the logging messages are only pushed
            to stdout. Otherwise, a logging `FileHandler` will write the logged messages
            to the given file path.
        n_jobs : int, optional
            The number of processes used for the feature calculation. If `None`, then
            the number returned by `os.cpu_count()` is used, by default None. \n
            If n_jobs is either 0 or 1, the code will be executed sequentially without
            creating a process pool. This is very useful when debugging, as the stack
            trace will be more comprehensible.

            .. tip::
                * It takes on avg. _300ms_ to schedule everything with
                  multiprocessing. So if your feature extraction code runs faster than
                  ~1.5s, it might not be worth it to parallelize the process
                  (and thus better set the `n_jobs` to 0-1).
                * This method its memory peaks are significantly lower when executed
                  sequentially. Set the `n_jobs` to 0-1 if this matters.

        Returns
        -------
        Union[List[pd.DataFrame], pd.DataFrame]
            The calculated features.

        Raises
        ------
        KeyError
            Raised when a required key is not found in `data`.

        &#34;&#34;&#34;
        # Delete other logging handlers
        delete_logging_handlers(logger)
        # Add logging handler (if path provided)
        if logging_file_path:
            add_logging_handler(logger, logging_file_path)

        # Convert the data to a series_dict
        series_dict: Dict[str, pd.Series] = {}
        for s in to_series_list(data):
            # Assert the assumptions we make!
            assert isinstance(s.index, pd.DatetimeIndex)
            assert s.index.is_monotonic_increasing

            if s.name in self.get_required_series():
                series_dict[str(s.name)] = s

        # Note: this variable has a global scope so this is shared in multiprocessing
        global get_stroll_func
        get_stroll_func = self._stroll_feat_generator(
            series_dict, window_idx, approve_sparsity
        )
        nb_stroll_funcs = self._get_stroll_feat_length()

        if n_jobs is None:
            n_jobs = os.cpu_count()
        n_jobs = min(n_jobs, nb_stroll_funcs)

        if n_jobs in [0, 1]:
            idxs = range(nb_stroll_funcs)
            if show_progress:
                idxs = tqdm(idxs)
            calculated_feature_list = [self._executor(idx) for idx in idxs]
        else:
            # https://pathos.readthedocs.io/en/latest/pathos.html#usage
            with ProcessPool(nodes=n_jobs, source=True) as pool:
                results = pool.uimap(
                    self._executor,
                    range(nb_stroll_funcs)
                )
                if show_progress:
                    results = tqdm(results, total=self._get_stroll_feat_length())
                calculated_feature_list = [f for f in results]
                # Close &amp; join - see: https://github.com/uqfoundation/pathos/issues/131
                pool.close()
                pool.join()
                # Clear because: https://github.com/uqfoundation/pathos/issues/111
                pool.clear()

        if return_df:
            return pd.concat(calculated_feature_list, axis=1, join=&#34;outer&#34;, copy=False)
        else:
            return calculated_feature_list

    def serialize(self, file_path: Union[str, Path]):
        &#34;&#34;&#34;Serialize this `FeatureCollection` instance.

        Parameters
        ----------
        file_path : Union[str, Path]
            The path where the `FeatureCollection` will be serialized.

        Notes
        -----
        * As we use [Dill](https://github.com/uqfoundation/dill){:target=&#34;_blank&#34;} to
          serialize the files, we can **also serialize functions which are defined in
          the local scope, like lambdas.**

        &#34;&#34;&#34;
        with open(file_path, &#34;wb&#34;) as f:
            dill.dump(self, f, recurse=True)

    def reduce(self, feat_cols_to_keep: List[str]) -&gt; FeatureCollection:
        &#34;&#34;&#34;Create a reduced FeatureCollection instance based on feat_cols_to_keep.

        For example, this is useful to optimize feature-extraction inference
        (for your selected features) after performing a feature-selection procedure.

        Parameters
        ----------
        feat_cols_to_keep: List[str]
            A subset of the feature collection instance its column names.
            This corresponds to the columns / names of the output from `calculate`
            method that you want to keep.

        Returns
        -------
        FeatureCollection
            A new FeatureCollection object, which only withholds the FeatureDescriptors
            which constitute the `feat_cols_to_keep` output.

        Note
        -----
        Some FeatureDescriptor objects may have multiple **output-names**.&lt;br&gt;
        Hence, if you only want to retain _a subset_ of that FeatureDescriptor its
        feature outputs, you will still get **all features** as the new
        FeatureCollection is constructed by applying a filter on de FeatureDescriptor
        list and we thus not alter these FeatureDescriptor objects themselves.

        &#34;&#34;&#34;
        # dict in which we store all the { output_col_name : (UUID, FeatureDescriptor) }
        # items of our current Featurecollection object
        feat_col_fd_mapping: Dict[str, Tuple[str, FeatureDescriptor]] = {}
        for (s_names, window, stride), fds in self._feature_desc_dict.items():
            fd: FeatureDescriptor
            for fd in fds:
                # As a single FeatureDescriptor can have multiple output col names, we
                # create a unique identifier for each FeatureDescriptor (on which we
                # will apply set-like operations later on to only retain all the unique
                # FeatureDescriptors)
                uuid_str = str(uuid.uuid4())
                for output_name in fd.function.output_names:
                    # Reconstruct the feature column name
                    feat_col_name = &#39;__&#39;.join([
                        &#39;|&#39;.join(s_names) if isinstance(s_names, tuple) else s_names,
                        output_name,
                        f&#39;w={timedelta_to_str(window)}_s={timedelta_to_str(stride)}&#39;
                    ])
                    feat_col_fd_mapping[feat_col_name] = (uuid_str, fd)

        assert all(fc in feat_col_fd_mapping for fc in feat_cols_to_keep)

        # Collect (uuid, FeatureDescriptor) for the feat_cols_to_keep
        fd_subset: List[Tuple[str, FeatureDescriptor]] = [
            feat_col_fd_mapping[fc] for fc in feat_cols_to_keep
        ]

        # Reduce to unique feature descriptor objects (based on uuid) and create a new
        # FeatureCollection for their deepcopy&#39;s.
        seen_uuids = set()
        return FeatureCollection(
            feature_descriptors=[
                deepcopy(unique_fd) for unique_fd in
                {
                    fd for (uuid_str, fd) in fd_subset
                    if uuid_str not in seen_uuids and not seen_uuids.add(uuid_str)
                }
            ]
        )

    def __repr__(self) -&gt; str:
        &#34;&#34;&#34;Representation string of a FeatureCollection.&#34;&#34;&#34;
        feature_keys = sorted(set(k[0] for k in self._feature_desc_dict.keys()))
        output_str = &#34;&#34;
        for feature_key in feature_keys:
            output_str += f&#34;{&#39;|&#39;.join(feature_key)}: (&#34;
            keys = (x for x in self._feature_desc_dict.keys() if x[0] == feature_key)
            for _, win_size, stride in keys:
                output_str += f&#34;\n\twin: &#34;
                win_str = timedelta_to_str(win_size)
                stride_str = timedelta_to_str(stride)
                output_str += f&#34;{str(win_str):&lt;6}, stride: {str(stride_str)}: [&#34;
                for feat_desc in self._feature_desc_dict[feature_key, win_size, stride]:
                    output_str += f&#34;\n\t\t{feat_desc._func_str},&#34;
                output_str += &#34;\n\t]&#34;
            output_str += &#34;\n)\n&#34;
        return output_str</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="tsflex.features.feature_collection.FeatureCollection.get_required_series"><code class="name flex">
<span>def <span class="ident">get_required_series</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return all required series names for this feature collection.</p>
<p>Return the list of series names that are required in order to calculate all the
features (defined by the <code>FeatureDescriptor</code> objects) of this feature
collection.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[str]</code></dt>
<dd>List of all the required series names.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_required_series(self) -&gt; List[str]:
    &#34;&#34;&#34;Return all required series names for this feature collection.

    Return the list of series names that are required in order to calculate all the
    features (defined by the `FeatureDescriptor` objects) of this feature
    collection.

    Returns
    -------
    List[str]
        List of all the required series names.

    &#34;&#34;&#34;
    return list(
        set(flatten([fr_key[0] for fr_key in self._feature_desc_dict.keys()]))
    )</code></pre>
</details>
</dd>
<dt id="tsflex.features.feature_collection.FeatureCollection.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self, features)</span>
</code></dt>
<dd>
<div class="desc"><p>Add feature(s) to the FeatureCollection.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>features</code></strong> :&ensp;<code>Union[FeatureDescriptor, MultipleFeatureDescriptors, <a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a>, List[Union[FeatureDescriptor, MultipleFeatureDescriptors, <a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a>]]]</code></dt>
<dd>Feature(s) (containers) whose contained features will be added.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>TypeError</code></dt>
<dd>Raised when an item within <code>features</code> is not an instance of
[<code>MultipleFeatureDescriptors</code>, <code>FeatureDescriptors</code>, <code><a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a></code>].</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add(
    self,
    features: Union[
        FeatureDescriptor, 
        MultipleFeatureDescriptors,
        FeatureCollection,
        List[
            Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection]
        ],
    ],
):
    &#34;&#34;&#34;Add feature(s) to the FeatureCollection.

    Parameters
    ----------
    features : Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection, List[Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection]]]
        Feature(s) (containers) whose contained features will be added.

    Raises
    ------
    TypeError
        Raised when an item within `features` is not an instance of
        [`MultipleFeatureDescriptors`, `FeatureDescriptors`, `FeatureCollection`].

    &#34;&#34;&#34;
    # Convert to list if necessary
    features = to_list(features)

    for feature in features:
        if isinstance(feature, MultipleFeatureDescriptors):
            self.add(feature.feature_descriptions)
        elif isinstance(feature, FeatureDescriptor):
            self._add_feature(feature)
        elif isinstance(feature, FeatureCollection):
            # List needs to be flattened
            self.add(list(flatten(feature._feature_desc_dict.values())))
        else:
            raise TypeError(f&#34;type: {type(feature)} is not supported - {feature}&#34;)</code></pre>
</details>
</dd>
<dt id="tsflex.features.feature_collection.FeatureCollection.calculate"><code class="name flex">
<span>def <span class="ident">calculate</span></span>(<span>self, data, return_df=False, window_idx='end', approve_sparsity=False, show_progress=False, logging_file_path=None, n_jobs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate features on the passed data.</p>
<h2 id="notes">Notes</h2>
<ul>
<li>The (column-)names of the series in <code>data</code> represent the names in the keys.</li>
<li>If a <code>logging_file_path</code> is provided, the execution (time) info can be
retrieved by calling <code>logger.get_feature_logs(logging_file_path)</code>.
Be aware that the <code>logging_file_path</code> gets cleared before the logger pushes
logged messages. Hence, one should use a separate logging file for each
constructed processing and feature instance with this library.</li>
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]</code></dt>
<dd>
<p>Dataframe or Series or list thereof, with all the required data for the
feature calculation. </p>
<p><strong>Remark</strong>: each Series / DataFrame must have a <code>pd.DatetimeIndex</code>.
<strong>Remark</strong>: we assume that each name / column is unique.</p>
</dd>
<dt><strong><code>return_df</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether the output needs to be a dataframe list or a DataFrame, by default
False.
If <code>True</code> the output dataframes will be merged to a DataFrame with an outer
merge.</dd>
<dt><strong><code>window_idx</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The window's index position which will be used as index for the
feature_window aggregation. Must be either of: ['begin', 'middle', 'end'],
by default 'end'. All features in this collection will use the same
window_idx.</dd>
<dt><strong><code>approve_sparsity</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Bool indicating whether the user acknowledges that there may be sparsity
(i.e., irregularly sampled data), by default False.
If False and sparsity is observed, a warning is raised.</dd>
<dt><strong><code>show_progress</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, the progress will be shown with a progressbar, by default False.</dd>
<dt><strong><code>logging_file_path</code></strong> :&ensp;<code>Union[str, Path]</code>, optional</dt>
<dd>The file path where the logged messages are stored. If <code>None</code>, then no
logging <code>FileHandler</code> will be used and the logging messages are only pushed
to stdout. Otherwise, a logging <code>FileHandler</code> will write the logged messages
to the given file path.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>
<p>The number of processes used for the feature calculation. If <code>None</code>, then
the number returned by <code>os.cpu_count()</code> is used, by default None. </p>
<p>If n_jobs is either 0 or 1, the code will be executed sequentially without
creating a process pool. This is very useful when debugging, as the stack
trace will be more comprehensible.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>It takes on avg. <em>300ms</em> to schedule everything with
multiprocessing. So if your feature extraction code runs faster than
~1.5s, it might not be worth it to parallelize the process
(and thus better set the <code>n_jobs</code> to 0-1).</li>
<li>This method its memory peaks are significantly lower when executed
sequentially. Set the <code>n_jobs</code> to 0-1 if this matters.</li>
</ul>
</div>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[List[pd.DataFrame], pd.DataFrame]</code></dt>
<dd>The calculated features.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>KeyError</code></dt>
<dd>Raised when a required key is not found in <code>data</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate(
    self,
    data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],
    return_df: Optional[bool] = False,
    window_idx: Optional[str] = &#34;end&#34;,
    approve_sparsity: Optional[bool] = False,
    show_progress: Optional[bool] = False,
    logging_file_path: Optional[Union[str, Path]] = None,
    n_jobs: Optional[int] = None,
) -&gt; Union[List[pd.DataFrame], pd.DataFrame]:
    &#34;&#34;&#34;Calculate features on the passed data.

    Notes
    ------
    * The (column-)names of the series in `data` represent the names in the keys.
    * If a `logging_file_path` is provided, the execution (time) info can be
      retrieved by calling `logger.get_feature_logs(logging_file_path)`.
      Be aware that the `logging_file_path` gets cleared before the logger pushes
      logged messages. Hence, one should use a separate logging file for each
      constructed processing and feature instance with this library.

    Parameters
    ----------
    data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]
        Dataframe or Series or list thereof, with all the required data for the
        feature calculation. \n
        **Remark**: each Series / DataFrame must have a `pd.DatetimeIndex`.
        **Remark**: we assume that each name / column is unique.
    return_df : bool, optional
        Whether the output needs to be a dataframe list or a DataFrame, by default
        False.
        If `True` the output dataframes will be merged to a DataFrame with an outer
        merge.
    window_idx : str, optional
        The window&#39;s index position which will be used as index for the
        feature_window aggregation. Must be either of: [&#39;begin&#39;, &#39;middle&#39;, &#39;end&#39;],
        by default &#39;end&#39;. All features in this collection will use the same
        window_idx.
    approve_sparsity: bool, optional
        Bool indicating whether the user acknowledges that there may be sparsity
        (i.e., irregularly sampled data), by default False.
        If False and sparsity is observed, a warning is raised.
    show_progress: bool, optional
        If True, the progress will be shown with a progressbar, by default False.
    logging_file_path : Union[str, Path], optional
        The file path where the logged messages are stored. If `None`, then no
        logging `FileHandler` will be used and the logging messages are only pushed
        to stdout. Otherwise, a logging `FileHandler` will write the logged messages
        to the given file path.
    n_jobs : int, optional
        The number of processes used for the feature calculation. If `None`, then
        the number returned by `os.cpu_count()` is used, by default None. \n
        If n_jobs is either 0 or 1, the code will be executed sequentially without
        creating a process pool. This is very useful when debugging, as the stack
        trace will be more comprehensible.

        .. tip::
            * It takes on avg. _300ms_ to schedule everything with
              multiprocessing. So if your feature extraction code runs faster than
              ~1.5s, it might not be worth it to parallelize the process
              (and thus better set the `n_jobs` to 0-1).
            * This method its memory peaks are significantly lower when executed
              sequentially. Set the `n_jobs` to 0-1 if this matters.

    Returns
    -------
    Union[List[pd.DataFrame], pd.DataFrame]
        The calculated features.

    Raises
    ------
    KeyError
        Raised when a required key is not found in `data`.

    &#34;&#34;&#34;
    # Delete other logging handlers
    delete_logging_handlers(logger)
    # Add logging handler (if path provided)
    if logging_file_path:
        add_logging_handler(logger, logging_file_path)

    # Convert the data to a series_dict
    series_dict: Dict[str, pd.Series] = {}
    for s in to_series_list(data):
        # Assert the assumptions we make!
        assert isinstance(s.index, pd.DatetimeIndex)
        assert s.index.is_monotonic_increasing

        if s.name in self.get_required_series():
            series_dict[str(s.name)] = s

    # Note: this variable has a global scope so this is shared in multiprocessing
    global get_stroll_func
    get_stroll_func = self._stroll_feat_generator(
        series_dict, window_idx, approve_sparsity
    )
    nb_stroll_funcs = self._get_stroll_feat_length()

    if n_jobs is None:
        n_jobs = os.cpu_count()
    n_jobs = min(n_jobs, nb_stroll_funcs)

    if n_jobs in [0, 1]:
        idxs = range(nb_stroll_funcs)
        if show_progress:
            idxs = tqdm(idxs)
        calculated_feature_list = [self._executor(idx) for idx in idxs]
    else:
        # https://pathos.readthedocs.io/en/latest/pathos.html#usage
        with ProcessPool(nodes=n_jobs, source=True) as pool:
            results = pool.uimap(
                self._executor,
                range(nb_stroll_funcs)
            )
            if show_progress:
                results = tqdm(results, total=self._get_stroll_feat_length())
            calculated_feature_list = [f for f in results]
            # Close &amp; join - see: https://github.com/uqfoundation/pathos/issues/131
            pool.close()
            pool.join()
            # Clear because: https://github.com/uqfoundation/pathos/issues/111
            pool.clear()

    if return_df:
        return pd.concat(calculated_feature_list, axis=1, join=&#34;outer&#34;, copy=False)
    else:
        return calculated_feature_list</code></pre>
</details>
</dd>
<dt id="tsflex.features.feature_collection.FeatureCollection.serialize"><code class="name flex">
<span>def <span class="ident">serialize</span></span>(<span>self, file_path)</span>
</code></dt>
<dd>
<div class="desc"><p>Serialize this <code><a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a></code> instance.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>Union[str, Path]</code></dt>
<dd>The path where the <code><a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a></code> will be serialized.</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li>As we use <a href="https://github.com/uqfoundation/dill" target="_blank">Dill</a> to
serialize the files, we can <strong>also serialize functions which are defined in
the local scope, like lambdas.</strong></li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def serialize(self, file_path: Union[str, Path]):
    &#34;&#34;&#34;Serialize this `FeatureCollection` instance.

    Parameters
    ----------
    file_path : Union[str, Path]
        The path where the `FeatureCollection` will be serialized.

    Notes
    -----
    * As we use [Dill](https://github.com/uqfoundation/dill){:target=&#34;_blank&#34;} to
      serialize the files, we can **also serialize functions which are defined in
      the local scope, like lambdas.**

    &#34;&#34;&#34;
    with open(file_path, &#34;wb&#34;) as f:
        dill.dump(self, f, recurse=True)</code></pre>
</details>
</dd>
<dt id="tsflex.features.feature_collection.FeatureCollection.reduce"><code class="name flex">
<span>def <span class="ident">reduce</span></span>(<span>self, feat_cols_to_keep)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a reduced FeatureCollection instance based on feat_cols_to_keep.</p>
<p>For example, this is useful to optimize feature-extraction inference
(for your selected features) after performing a feature-selection procedure.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>feat_cols_to_keep</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>A subset of the feature collection instance its column names.
This corresponds to the columns / names of the output from <code>calculate</code>
method that you want to keep.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a></code></dt>
<dd>A new FeatureCollection object, which only withholds the FeatureDescriptors
which constitute the <code>feat_cols_to_keep</code> output.</dd>
</dl>
<h2 id="note">Note</h2>
<p>Some FeatureDescriptor objects may have multiple <strong>output-names</strong>.<br>
Hence, if you only want to retain <em>a subset</em> of that FeatureDescriptor its
feature outputs, you will still get <strong>all features</strong> as the new
FeatureCollection is constructed by applying a filter on de FeatureDescriptor
list and we thus not alter these FeatureDescriptor objects themselves.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reduce(self, feat_cols_to_keep: List[str]) -&gt; FeatureCollection:
    &#34;&#34;&#34;Create a reduced FeatureCollection instance based on feat_cols_to_keep.

    For example, this is useful to optimize feature-extraction inference
    (for your selected features) after performing a feature-selection procedure.

    Parameters
    ----------
    feat_cols_to_keep: List[str]
        A subset of the feature collection instance its column names.
        This corresponds to the columns / names of the output from `calculate`
        method that you want to keep.

    Returns
    -------
    FeatureCollection
        A new FeatureCollection object, which only withholds the FeatureDescriptors
        which constitute the `feat_cols_to_keep` output.

    Note
    -----
    Some FeatureDescriptor objects may have multiple **output-names**.&lt;br&gt;
    Hence, if you only want to retain _a subset_ of that FeatureDescriptor its
    feature outputs, you will still get **all features** as the new
    FeatureCollection is constructed by applying a filter on de FeatureDescriptor
    list and we thus not alter these FeatureDescriptor objects themselves.

    &#34;&#34;&#34;
    # dict in which we store all the { output_col_name : (UUID, FeatureDescriptor) }
    # items of our current Featurecollection object
    feat_col_fd_mapping: Dict[str, Tuple[str, FeatureDescriptor]] = {}
    for (s_names, window, stride), fds in self._feature_desc_dict.items():
        fd: FeatureDescriptor
        for fd in fds:
            # As a single FeatureDescriptor can have multiple output col names, we
            # create a unique identifier for each FeatureDescriptor (on which we
            # will apply set-like operations later on to only retain all the unique
            # FeatureDescriptors)
            uuid_str = str(uuid.uuid4())
            for output_name in fd.function.output_names:
                # Reconstruct the feature column name
                feat_col_name = &#39;__&#39;.join([
                    &#39;|&#39;.join(s_names) if isinstance(s_names, tuple) else s_names,
                    output_name,
                    f&#39;w={timedelta_to_str(window)}_s={timedelta_to_str(stride)}&#39;
                ])
                feat_col_fd_mapping[feat_col_name] = (uuid_str, fd)

    assert all(fc in feat_col_fd_mapping for fc in feat_cols_to_keep)

    # Collect (uuid, FeatureDescriptor) for the feat_cols_to_keep
    fd_subset: List[Tuple[str, FeatureDescriptor]] = [
        feat_col_fd_mapping[fc] for fc in feat_cols_to_keep
    ]

    # Reduce to unique feature descriptor objects (based on uuid) and create a new
    # FeatureCollection for their deepcopy&#39;s.
    seen_uuids = set()
    return FeatureCollection(
        feature_descriptors=[
            deepcopy(unique_fd) for unique_fd in
            {
                fd for (uuid_str, fd) in fd_subset
                if uuid_str not in seen_uuids and not seen_uuids.add(uuid_str)
            }
        ]
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<div style="text-align: center">
<a class="homelink" rel="home" title="tsflex home" href="/tsflex/">
<img src="https://cdn.discordapp.com/attachments/372491075153166338/852904976560554094/logo.png"
alt="logo should be displayed here" style="width: 100%;"></a>
</div>
</header>
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#see-also">See Also</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tsflex.features" href="index.html">tsflex.features</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a></code></h4>
<ul class="">
<li><code><a title="tsflex.features.feature_collection.FeatureCollection.get_required_series" href="#tsflex.features.feature_collection.FeatureCollection.get_required_series">get_required_series</a></code></li>
<li><code><a title="tsflex.features.feature_collection.FeatureCollection.add" href="#tsflex.features.feature_collection.FeatureCollection.add">add</a></code></li>
<li><code><a title="tsflex.features.feature_collection.FeatureCollection.calculate" href="#tsflex.features.feature_collection.FeatureCollection.calculate">calculate</a></code></li>
<li><code><a title="tsflex.features.feature_collection.FeatureCollection.serialize" href="#tsflex.features.feature_collection.FeatureCollection.serialize">serialize</a></code></li>
<li><code><a title="tsflex.features.feature_collection.FeatureCollection.reduce" href="#tsflex.features.feature_collection.FeatureCollection.reduce">reduce</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>