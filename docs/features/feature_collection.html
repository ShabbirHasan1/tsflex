<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>tsflex.features.feature_collection API documentation</title>
<meta name="description" content="FeatureCollection class for bookkeeping and calculation of time-series features â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/foundation.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em;padding-left:1em;padding-right:1em}button{display:none}#sidebar{padding:3px;max-width:20em;overflow:hidden;min-width:19.8em}#sidebar > *:last-child{margin-bottom:1cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;border-top:1px solid #ddd;text-align:right}#footer p{}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f1f3f9;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:0.5em;padding:0px}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;max_width:100%;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.info{background:#edfcf4}.admonition.note,.admonition.important{background:#ebf3ff}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#edfcf4}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#ffddcc}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:850px){.sidebar_container{display:flex;transition:0.75s ease}.sidebar_small{width:0;margin:0;padding:0}.hide_content{display:none}button{display:initial;float:left;position:sticky;border:none;height:5ch;width:5ch;border-radius:50%;box-shadow:0px 1px 4px 1px rgba(0,0,0,.2);top:5%;left:100%;transform:translateX(-50%);cursor:pointer}#sidebar{width:25%;height:100vh;overflow:auto;position:sticky;top:0;transition:0.75s ease}#index_button_img{opacity:0.65}#content{max-width:105ch;padding:2em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1em;padding-right:0.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-212611910-1"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-212611910-1');
</script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="https://media.discordapp.net/attachments/372491075153166338/852906324417445908/icon.png">
</head>
<body>
<main>
<article id="content">
<button id="index_button_button"><img id="index_button_img"
src="https://image.flaticon.com/icons/png/512/56/56763.png"
alt="" width="33" height="25"></button>
<header>
<h1 class="title">Module <code>tsflex.features.feature_collection</code></h1>
</header>
<section id="section-intro">
<p>FeatureCollection class for bookkeeping and calculation of time-series features.</p>
<p>Methods, next to <code><a title="tsflex.features.feature_collection.FeatureCollection.calculate" href="#tsflex.features.feature_collection.FeatureCollection.calculate">.calculate()</a></code>, worth looking at: </p>
<ul>
<li><code><a title="tsflex.features.feature_collection.FeatureCollection.serialize" href="#tsflex.features.feature_collection.FeatureCollection.serialize">.serialize()</a></code> - serialize the FeatureCollection to a file</li>
<li><code><a title="tsflex.features.feature_collection.FeatureCollection.reduce" href="#tsflex.features.feature_collection.FeatureCollection.reduce">.reduce()</a></code> - reduce the number of features after feature selection</li>
</ul>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;FeatureCollection class for bookkeeping and calculation of time-series features.

Methods, next to `FeatureCollection.calculate()`, worth looking at: \n
* `FeatureCollection.serialize()` - serialize the FeatureCollection to a file
* `FeatureCollection.reduce()` - reduce the number of features after feature selection

&#34;&#34;&#34;

from __future__ import annotations

import warnings

__author__ = &#34;Jonas Van Der Donckt, Emiel Deprost, Jeroen Van Der Donckt&#34;

import os
import traceback
import uuid
from copy import deepcopy
from pathlib import Path
from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union

import dill
import numpy as np
import pandas as pd
from multiprocess import Pool
from tqdm.auto import tqdm

from ..features.function_wrapper import FuncWrapper
from ..utils.attribute_parsing import AttributeParser
from ..utils.data import flatten, to_list, to_series_list
from ..utils.logging import add_logging_handler, delete_logging_handlers
from ..utils.time import parse_time_arg, timedelta_to_str
from .feature import FeatureDescriptor, MultipleFeatureDescriptors
from .logger import logger
from .segmenter import StridedRolling, StridedRollingFactory
from .utils import _check_start_end_array, _determine_bounds


class FeatureCollection:
    &#34;&#34;&#34;Create a FeatureCollection.

    Parameters
    ----------
    feature_descriptors : Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection, List[Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection]]], optional
        Initial (list of) feature(s) to add to collection, by default None

    Notes
    -----
    * The `series_name` property of the `FeatureDescriptor`s should **not withhold a &#34;|&#34;
      character**, since &#34;|&#34; is used to join the series names of features which use
      multiple series as input).&lt;br&gt;
      e.g.&lt;br&gt;
        * `ACC|x` is **not** allowed as series name, as this is ambiguous and could
          represent that this feature is constructed with a combination of the `ACC`
          and `x` signal.&lt;br&gt;
          Note that `max|feat` is allowed as feature output name.
    * Both the `series_name` and `output_name` property of the `FeatureDescriptor`s
      **should not withhold &#34;__&#34;** in its string representations. This constraint is
      mainly made for readability purposes.

    The two statements above will be asserted

    &#34;&#34;&#34;

    def __init__(
        self,
        feature_descriptors: Optional[
            Union[
                FeatureDescriptor,
                MultipleFeatureDescriptors,
                FeatureCollection,
                List[
                    Union[
                        FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection
                    ]
                ],
            ]
        ] = None,
    ):
        # The feature collection is a dict with keys of type:
        #   tuple(tuple(str), float OR pd.timedelta)
        # The outer tuple&#39;s values correspond to (series_key(s), window)
        self._feature_desc_dict: Dict[
            Tuple[Tuple[str, ...], Union[float, pd.Timedelta]], List[FeatureDescriptor]
        ] = {}

        if feature_descriptors:
            self.add(feature_descriptors)

    def get_required_series(self) -&gt; List[str]:
        &#34;&#34;&#34;Return all required series names for this feature collection.

        Return the list of series names that are required in order to calculate all the
        features (defined by the `FeatureDescriptor` objects) of this feature
        collection.

        Returns
        -------
        List[str]
            List of all the required series names.

        &#34;&#34;&#34;
        return list(
            set(flatten([fr_key[0] for fr_key in self._feature_desc_dict.keys()]))
        )

    def get_nb_output_features(self) -&gt; int:
        &#34;&#34;&#34;Return the number of output features in this feature collection.

        Returns
        -------
        int
            The number of output features in this feature collection.

        &#34;&#34;&#34;
        fd_list: Iterable[FeatureDescriptor] = flatten(self._feature_desc_dict.values())
        return sum(fd.get_nb_output_features() for fd in fd_list)

    def _get_nb_output_features_without_window(self) -&gt; int:
        &#34;&#34;&#34;Return the number of output features in this feature collection, without
        using the window as a unique identifier.

        This is relevant for when the window value(s) are overridden by passing
        `segment_start_idxs` and `segment_end_idxs` to the `calculate` method.

        Returns
        -------
        int:
            The number of output features in this feature collection without using the
            window as a unique identifier.

        &#34;&#34;&#34;
        return len(
            set(
                (series, o)
                for (series, _), fd_list in self._feature_desc_dict.items()
                for fd in fd_list
                for o in fd.function.output_names
            )
        )

    @staticmethod
    def _get_collection_key(
        feature: FeatureDescriptor,
    ) -&gt; Tuple[Tuple[str, ...], Union[pd.Timedelta, float, None]]:
        # Note: `window` property can be either a pd.Timedelta or a float or None
        # assert feature.window is not None
        return feature.series_name, feature.window

    def _check_feature_descriptors(
        self,
        skip_none: bool,
        calc_stride: Optional[Union[float, pd.Timedelta, None]] = None,
    ):
        &#34;&#34;&#34;Verify whether all added FeatureDescriptors imply the same-input data type.

        If this condition is not met, a warning will be raised.

        Parameters
        ----------
        skip_none: bool
            Whether to include None stride values in the checks.
        calc_stride: Union[float, pd.Timedelta, None], optional
            The `FeatureCollection.calculate` its stride argument, by default None.
            This stride takes precedence over a `FeatureDescriptor` its stride when
            it is not None.

        &#34;&#34;&#34;
        dtype_set = set()
        for series_names, win in self._feature_desc_dict.keys():
            for fd in self._feature_desc_dict[(series_names, win)]:
                stride = calc_stride if calc_stride is not None else fd.stride
                if skip_none and stride is None:
                    dtype_set.add(AttributeParser.determine_type(win))
                else:
                    dtype_set.add(
                        AttributeParser.determine_type([win] + to_list(stride))
                    )

        if len(dtype_set) &gt; 1:
            warnings.warn(
                &#34;There are multiple FeatureDescriptor window-stride &#34;
                + f&#34;datatypes present in this FeatureCollection, i.e.: {dtype_set}&#34;,
                category=RuntimeWarning,
            )

    def _add_feature(self, feature: FeatureDescriptor):
        &#34;&#34;&#34;Add a `FeatureDescriptor` instance to the collection.

        Parameters
        ----------
        feature : FeatureDescriptor
            The feature that will be added to this feature collection.

        &#34;&#34;&#34;
        # Check whether the `|` is not present in the series
        assert not any(&#34;|&#34; in s_name for s_name in feature.get_required_series())
        # Check whether the &#39;__&#34; is not present in the series and function output names
        assert not any(
            &#34;__&#34; in output_name for output_name in feature.function.output_names
        )
        assert not any(&#34;__&#34; in s_name for s_name in feature.get_required_series())

        series_win_stride_key = self._get_collection_key(feature)
        if series_win_stride_key in self._feature_desc_dict.keys():
            added_output_names = flatten(
                f.function.output_names
                for f in self._feature_desc_dict[series_win_stride_key]
            )
            # Check that not a feature with the same output_name(s) is already added
            # for the series_win_stride_key
            assert not any(
                output_name in added_output_names
                for output_name in feature.function.output_names
            )
            self._feature_desc_dict[series_win_stride_key].append(feature)
        else:
            self._feature_desc_dict[series_win_stride_key] = [feature]

    def add(
        self,
        features: Union[
            FeatureDescriptor,
            MultipleFeatureDescriptors,
            FeatureCollection,
            List[
                Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection]
            ],
        ],
    ):
        &#34;&#34;&#34;Add feature(s) to the FeatureCollection.

        Parameters
        ----------
        features : Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection, List[Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection]]]
            Feature(s) (containers) whose contained features will be added.

        Raises
        ------
        TypeError
            Raised when an item within `features` is not an instance of
            [`MultipleFeatureDescriptors`, `FeatureDescriptors`, `FeatureCollection`].

        &#34;&#34;&#34;
        # Convert to list if necessary
        features = to_list(features)

        for feature in features:
            if isinstance(feature, MultipleFeatureDescriptors):
                self.add(feature.feature_descriptions)
            elif isinstance(feature, FeatureDescriptor):
                self._add_feature(feature)
            elif isinstance(feature, FeatureCollection):
                # List needs to be flattened
                self.add(list(flatten(feature._feature_desc_dict.values())))
            else:
                raise TypeError(f&#34;type: {type(feature)} is not supported - {feature}&#34;)

        # After adding the features, check whether the descriptors are compatible
        self._check_feature_descriptors(skip_none=True)

    @staticmethod
    def _executor(idx: int):
        # global get_stroll_func
        stroll, function = get_stroll_func(idx)
        return stroll.apply_func(function)

    # def _get_stroll(self, kwargs):
    #     return StridedRollingFactory.get_segmenter(**kwargs)

    def _stroll_feat_generator(
        self,
        series_dict: Dict[str, pd.Series],
        calc_stride: Union[List[Union[float, pd.Timedelta]], None],
        segment_start_idxs: Union[np.ndarray, None],
        segment_end_idxs: Union[np.ndarray, None],
        start_idx: Any,
        end_idx: Any,
        window_idx: str,
        include_final_window: bool,
        approve_sparsity: bool,
    ) -&gt; Callable[[int], Tuple[StridedRolling, FuncWrapper]]:
        # --- Future work ---
        # We could also make the StridedRolling creation multithreaded
        # Very low priority because the STROLL __init__ is rather efficient!
        keys_wins_strides = list(self._feature_desc_dict.keys())
        lengths = np.cumsum(
            [len(self._feature_desc_dict[k]) for k in keys_wins_strides]
        )

        def get_stroll_function(idx) -&gt; Tuple[StridedRolling, FuncWrapper]:
            key_idx = np.searchsorted(lengths, idx, &#34;right&#34;)  # right bc idx starts at 0
            key, win = keys_wins_strides[key_idx]

            feature = self._feature_desc_dict[keys_wins_strides[key_idx]][
                idx - lengths[key_idx]
            ]
            stride = feature.stride if calc_stride is None else calc_stride
            function: FuncWrapper = feature.function
            # The factory method will instantiate the right StridedRolling object
            stroll_arg_dict = dict(
                data=[series_dict[k] for k in key],
                window=win,
                strides=stride,
                segment_start_idxs=segment_start_idxs,
                segment_end_idxs=segment_end_idxs,
                start_idx=start_idx,
                end_idx=end_idx,
                window_idx=window_idx,
                include_final_window=include_final_window,
                approve_sparsity=approve_sparsity,
                func_data_type=function.input_type,
            )
            stroll = StridedRollingFactory.get_segmenter(**stroll_arg_dict)
            return stroll, function

        return get_stroll_function

    def _get_stroll_feat_length(self) -&gt; int:
        return sum(
            len(self._feature_desc_dict[k]) for k in self._feature_desc_dict.keys()
        )

    def _check_no_multiple_windows(self):
        assert (
            self._get_nb_output_features_without_window()
            == self.get_nb_output_features()
        ), (
            &#34;When using `segment_XXX_idxs`; each output name - series_input combination&#34;
            + &#34; can only have 1 window (or None)&#34;
        )

    @staticmethod
    def _process_segment_idxs(
        segment_idxs: Union[list, np.ndarray, pd.Series, pd.Index]
    ) -&gt; np.ndarray:
        if hasattr(segment_idxs, &#34;values&#34;):
            segment_idxs = segment_idxs.values
        segment_idxs = np.asarray(segment_idxs)
        if segment_idxs.ndim &gt; 1:
            segment_idxs = segment_idxs.squeeze()  # remove singleton dimensions
        return segment_idxs

    def calculate(
        self,
        data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],
        stride: Optional[Union[float, str, pd.Timedelta, List, None]] = None,
        segment_start_idxs: Optional[
            Union[list, np.ndarray, pd.Series, pd.Index]
        ] = None,
        segment_end_idxs: Optional[Union[list, np.ndarray, pd.Series, pd.Index]] = None,
        return_df: Optional[bool] = False,
        window_idx: Optional[str] = &#34;end&#34;,
        include_final_window: Optional[bool] = False,
        bound_method: Optional[str] = &#34;inner&#34;,
        approve_sparsity: Optional[bool] = False,
        show_progress: Optional[bool] = False,
        logging_file_path: Optional[Union[str, Path]] = None,
        n_jobs: Optional[int] = None,
    ) -&gt; Union[List[pd.DataFrame], pd.DataFrame]:
        &#34;&#34;&#34;Calculate features on the passed data.

        Parameters
        ----------
        data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]
            Dataframe or Series or list thereof, with all the required data for the
            feature calculation. \n
            **Assumptions**: \n
            * each Series / DataFrame must have a sortable index. This index represents
            the sequence position of the corresponding values, the index can be either
            numeric or a ``pd.DatetimeIndex``.
            * each Series / DataFrame index must be comparable with all others
            * we assume that each series-name / dataframe-column-name is unique.
        stride: Union[float, str, pd.Timedelta, List[Union[float, str, pd.Timedelta], None], optional
            The stride size. By default None. This argument supports multiple types: \n
            * If None, the stride of the `FeatureDescriptor` objects will be used.
            * If the type is an `float` or an `int`, its value represents the series:\n
                - its stride **range** when a **non time-indexed** series is passed.
                - the stride in **number of samples**, when a **time-indexed** series
                is passed (must then be and `int`)
            * If the stride&#39;s type is a `pd.Timedelta`, the stride size represents
            the stride-time delta. The passed data **must have a time-index**.
            * If a `str`, it must represent a stride-time-delta-string. Hence, the
            **passed data must have a time-index**. \n
            .. Note::
                When set, this stride argument takes precedence over the stride property
                of the `FeatureDescriptor`s in this `FeatureCollection` (i.e., when a
                not None value for `stride` passed to this method).
        segment_start_idxs: Union[list, np.ndarray, pd.Series, pd.Index], optional
            The start indices of the segments. If None, the start indices will be
            computed from the data using either:\n
            - the `segment_end_idxs` - the `window` size property of the
                `FeatureDescriptor` in this `FeatureCollection` (if `segment_end_idxs`
                is not None)
            - strided-window rolling on the data using `window` and `stride` of the
                `FeatureDescriptor` in this `FeatureCollection` (if `segment_end_idxs`
                 is also None). (Note that the `stride` argument of this method takes
                 precedence over the `stride` property of the `FeatureDescriptor`s).
            By default None.
        segment_end_idxs: Union[list, np.ndarray, pd.Series, pd.Index], optional
            The end indices for the segmented windows. If None, the end indices will be
            computed from the data using either:\n
            - the `segment_start_idxs` + the `window` size property of the
                `FeatureDescriptor` in this `FeatureCollection` (if `segment_start_idxs`
                is not None)
            - strided-window rolling on the data using `window` and `stride` of the
                `FeatureDescriptor` in this `FeatureCollection` (if `segment_start_idxs`
                 is also None). (Note that the `stride` argument of this method takes
                 precedence over the `stride` property of the `FeatureDescriptor`s).
            By default None.

            ..Note::
                When passing both `segment_start_idxs` and `segment_end_idxs`, these two
                arguments must have the same length and every start index must be &lt;=
                than the corresponding end index.
                Note that passing both arguments, discards any meaning of the `window`
                and `stride` values (as these segment indices define the segmented data,
                and thus no strided-window rolling index calculation has to be executed).
                As such, the user can create variable-length segmented windows. However,
                in such cases, the user should be weary that the feature functions are
                invariant to these (potentially variable-length) windows.
        return_df : bool, optional
            Whether the output needs to be a DataFrame or a list thereof, by default
            False. If `True` the output dataframes will be merged to a DataFrame with an
            outer merge.
        window_idx : str, optional
            The window&#39;s index position which will be used as index for the
            feature_window aggregation. Must be either of: `[&#34;begin&#34;, &#34;middle&#34;, &#34;end&#34;]`.
            by **default &#34;end&#34;**. All features in this collection will use the same
            window_idx.

            ..Note::
                `window_idx`=&#34;end&#34; uses the window&#39;s end (= right open bound) as
                output index. \n
                `window_idx`=&#34;begin&#34; uses the window&#39;s start idx (= left closed bound)
                as output index.
        include_final_window : bool, optional
            Whether the final (possibly incomplete) window should be included in the
            strided-window segmentation, by default False.

            .. Note::
                The remarks below apply when `include_final_window` is set to True.
                The user should be aware that the last window *might* be incomplete,
                i.e.;

                - when equally sampled, the last window *might* be smaller than the
                  the other windows.
                - when not equally sampled, the last window *might* not include all the
                  data points (as the begin-time + window-size comes after the last data
                  point).

                Note, that when equally sampled, the last window *will* be a full window
                when:

                - the stride is the sampling rate of the data (or stride = 1 for
                sample-based configurations).&lt;br&gt;
                **Remark**: that when `include_final_window` is set to False, the last
                window (which is a full) window will not be included!
                - *(len * sampling_rate - window_size) % stride = 0*. Remark that the
                  above case is a base case of this.
        bound_method: str, optional
            The start-end bound methodology which is used to generate the slice ranges
            when ``data`` consists of multiple series / columns.
            Must be either of: `[&#34;inner&#34;, &#34;inner-outer&#34;, &#34;outer&#34;]`, by default &#34;inner&#34;.

            * if ``inner``, the inner-bounds of the series are returned.
            * if ``inner-outer``, the left-inner and right-outer bounds of the series
              are returned.
            * if ``outer``, the outer-bounds of the series are returned.
        approve_sparsity: bool, optional
            Bool indicating whether the user acknowledges that there may be sparsity
            (i.e., irregularly sampled data), by default False.
            If False and sparsity is observed, a warning is raised.
        show_progress: bool, optional
            If True, the progress will be shown with a progressbar, by default False.
        logging_file_path : Union[str, Path], optional
            The file path where the logged messages are stored. If `None`, then no
            logging `FileHandler` will be used and the logging messages are only pushed
            to stdout. Otherwise, a logging `FileHandler` will write the logged messages
            to the given file path. See also the `tsflex.features.logger` module.
        n_jobs : int, optional
            The number of processes used for the feature calculation. If `None`, then
            the number returned by _os.cpu_count()_ is used, by default None. \n
            If n_jobs is either 0 or 1, the code will be executed sequentially without
            creating a process pool. This is very useful when debugging, as the stack
            trace will be more comprehensible.
            .. note::
                Multiprocessed execution is not supported on Windows. Even when,
                `n_jobs` is set &gt; 1, the feature extraction will still be executed
                sequentially.
                Why do we not support multiprocessing on Windows; see this issue
                https://github.com/predict-idlab/tsflex/issues/51

            .. tip::
                It takes on avg. _300ms_ to schedule everything with
                multiprocessing. So if your sequential feature extraction code runs
                faster than ~1s, it might not be worth it to parallelize the process
                (and thus better leave `n_jobs` to 0 or 1).

        Returns
        -------
        Union[List[pd.DataFrame], pd.DataFrame]
            The calculated features.

        Raises
        ------
        KeyError
            Raised when a required key is not found in `data`.

        Notes
        ------
        * The (column-)names of the series in `data` represent the `series_names`.
        * If a `logging_file_path` is provided, the execution (time) info can be
          retrieved by calling `logger.get_feature_logs(logging_file_path)`.
          Be aware that the `logging_file_path` gets cleared before the logger pushes
          logged messages. Hence, one should use a separate logging file for each
          constructed processing and feature instance with this library.


        &#34;&#34;&#34;
        # Delete other logging handlers
        delete_logging_handlers(logger)
        # Add logging handler (if path provided)
        if logging_file_path:
            f_handler = add_logging_handler(logger, logging_file_path)

        # Convert to numpy array (if necessary)
        if segment_start_idxs is not None:
            segment_start_idxs = FeatureCollection._process_segment_idxs(
                segment_start_idxs
            )
        if segment_end_idxs is not None:
            segment_end_idxs = FeatureCollection._process_segment_idxs(segment_end_idxs)

        if segment_start_idxs is not None and segment_end_idxs is not None:
            # Check if segment indices have same length and whether every start idx
            # &lt;= end idx
            _check_start_end_array(segment_start_idxs, segment_end_idxs)
            # Check if there is either 1 or No(ne) window value for every output name -
            # input_series combination
            self._check_no_multiple_windows()

        if segment_start_idxs is None or segment_end_idxs is None:
            assert all(
                fd.window is not None
                for fd in flatten(self._feature_desc_dict.values())
            ), (
                &#34;Each feature descriptor must have a window when not both &#34;
                + &#34;segment_start_idxs and segment_end_idxs are provided&#34;
            )

        if stride is None and segment_start_idxs is None and segment_end_idxs is None:
            assert all(
                fd.stride is not None
                for fd in flatten(self._feature_desc_dict.values())
            ), (
                &#34;Each feature descriptor must have a stride when no stride or &#34;
                + &#34;segment indices are passed to this method!&#34;
            )
        elif stride is not None and (
            segment_start_idxs is not None or segment_end_idxs is not None
        ):
            raise ValueError(
                &#34;The stride and any segment index argument cannot be set together! &#34;
                + &#34;At least one of both should be None.&#34;
            )

        if stride is not None:
            # Verify whether the stride complies with the input data dtype
            stride = [
                parse_time_arg(s) if isinstance(s, str) else s for s in to_list(stride)
            ]
            self._check_feature_descriptors(skip_none=False, calc_stride=stride)

        # Convert the data to a series_dict
        series_dict: Dict[str, pd.Series] = {}
        for s in to_series_list(data):
            if not s.index.is_monotonic_increasing:
                warnings.warn(
                    f&#34;The index of series &#39;{s.name}&#39; is not monotonic increasing. &#34;
                    + &#34;The series will be sorted by the index.&#34;,
                    RuntimeWarning,
                )
                s = s.sort_index(ascending=True, inplace=False, ignore_index=False)

            # Assert the assumptions we make!
            assert s.index.is_monotonic_increasing

            if s.name in self.get_required_series():
                series_dict[str(s.name)] = s

        # Determine the bounds of the series dict items and slice on them
        # TODO: is dit wel nodig `hier? want we doen dat ook in de strided rolling
        start, end = _determine_bounds(bound_method, list(series_dict.values()))
        series_dict = {
            n: s.loc[
                s.index.dtype.type(start) : s.index.dtype.type(end)
            ]  # TODO: check memory efficiency of ths
            for n, s, in series_dict.items()
        }

        # Note: this variable has a global scope so this is shared in multiprocessing
        # TODO: try to make this more efficient (but is not really the bottleneck)
        global get_stroll_func
        get_stroll_func = self._stroll_feat_generator(
            series_dict,
            calc_stride=stride,
            segment_start_idxs=segment_start_idxs,
            segment_end_idxs=segment_end_idxs,
            start_idx=start,
            end_idx=end,
            window_idx=window_idx,
            include_final_window=include_final_window,
            approve_sparsity=approve_sparsity,
        )
        nb_stroll_funcs = self._get_stroll_feat_length()

        if (
            os.name == &#34;nt&#34;
        ):  # On Windows no multiprocessing is supported, see https://github.com/predict-idlab/tsflex/issues/51
            n_jobs = 1
        elif n_jobs is None:
            n_jobs = os.cpu_count()
        n_jobs = min(n_jobs, nb_stroll_funcs)

        calculated_feature_list = None
        if n_jobs in [0, 1]:
            idxs = range(nb_stroll_funcs)
            if show_progress:
                idxs = tqdm(idxs)
            try:
                calculated_feature_list = [self._executor(idx) for idx in idxs]
            except Exception:
                traceback.print_exc()
        else:
            with Pool(processes=n_jobs) as pool:
                results = pool.imap_unordered(self._executor, range(nb_stroll_funcs))
                if show_progress:
                    results = tqdm(results, total=nb_stroll_funcs)
                try:
                    calculated_feature_list = [f for f in results]
                except Exception:
                    traceback.print_exc()
                    pool.terminate()
                finally:
                    # Close &amp; join because: https://github.com/uqfoundation/pathos/issues/131
                    pool.close()
                    pool.join()

        # Close the file handler (this avoids PermissionError: [WinError 32])
        if logging_file_path:
            f_handler.close()
            logger.removeHandler(f_handler)

        if calculated_feature_list is None:
            raise RuntimeError(
                &#34;Feature Extraction halted due to error while extracting one &#34;
                + &#34;(or multiple) feature(s)! See stack trace above.&#34;
            )

        if return_df:
            # concatenate &amp; sort the columns
            df = pd.concat(calculated_feature_list, axis=1, join=&#34;outer&#34;, copy=False)
            return df.reindex(sorted(df.columns), axis=1)
        else:
            return calculated_feature_list

    def serialize(self, file_path: Union[str, Path]):
        &#34;&#34;&#34;Serialize this FeatureCollection instance.

        Parameters
        ----------
        file_path : Union[str, Path]
            The path where the `FeatureCollection` will be serialized.

        Note
        -----
        As we use [Dill](https://github.com/uqfoundation/dill){:target=&#34;_blank&#34;} to
        serialize the files, we can **also serialize functions which are defined in
        the local scope, like lambdas.**

        &#34;&#34;&#34;
        with open(file_path, &#34;wb&#34;) as f:
            dill.dump(self, f, recurse=True)

    def reduce(self, feat_cols_to_keep: List[str]) -&gt; FeatureCollection:
        &#34;&#34;&#34;Create a reduced FeatureCollection instance based on `feat_cols_to_keep`.

        For example, this is useful to optimize feature-extraction inference
        (for your selected features) after performing a feature-selection procedure.

        Parameters
        ----------
        feat_cols_to_keep: List[str]
            A subset of the feature collection instance its column names.
            This corresponds to the columns / names of the output from `calculate`
            method that you want to keep.

        Returns
        -------
        FeatureCollection
            A new FeatureCollection object, which only withholds the FeatureDescriptors
            which constitute the `feat_cols_to_keep` output.

        Note
        ----
        Some FeatureDescriptor objects may have multiple **output-names**.&lt;br&gt;
        Hence, if you only want to retain _a subset_ of that FeatureDescriptor its
        feature outputs, you will still get **all features** as the new
        FeatureCollection is constructed by applying a filter on de FeatureDescriptor
        list and we thus not alter these FeatureDescriptor objects themselves.

        &#34;&#34;&#34;
        # dict in which we store all the { output_col_name : (UUID, FeatureDescriptor) }
        # items of our current FeatureCollection object
        manual_window = False
        if any(c.endswith(&#34;w=manual&#34;) for c in feat_cols_to_keep):
            assert all(c.endswith(&#34;w=manual&#34;) for c in feat_cols_to_keep)
            # As the windows are created manual, the FeatureCollection cannot contain
            # multiple windows for the same output name - input_series combination
            self._check_no_multiple_windows()
            manual_window = True
        feat_col_fd_mapping: Dict[str, Tuple[str, FeatureDescriptor]] = {}
        for (s_names, window), fd_list in self._feature_desc_dict.items():
            window = &#34;manual&#34; if manual_window else self._ws_to_str(window)
            for fd in fd_list:
                # As a single FeatureDescriptor can have multiple output col names, we
                # create a unique identifier for each FeatureDescriptor (on which we
                # will apply set-like operations later on to only retain all the unique
                # FeatureDescriptors)
                uuid_str = str(uuid.uuid4())
                for output_name in fd.function.output_names:
                    # Reconstruct the feature column name
                    feat_col_name = StridedRolling.construct_output_index(
                        series_keys=s_names, feat_name=output_name, win_str=window
                    )
                    feat_col_fd_mapping[feat_col_name] = (uuid_str, fd)

        assert all(fc in feat_col_fd_mapping for fc in feat_cols_to_keep)

        # Collect (uuid, FeatureDescriptor) for the feat_cols_to_keep
        fd_subset: List[Tuple[str, FeatureDescriptor]] = [
            feat_col_fd_mapping[fc] for fc in feat_cols_to_keep
        ]

        # Reduce to unique feature descriptor objects (based on uuid) and create a new
        # FeatureCollection for their deepcopy&#39;s.
        seen_uuids = set()
        return FeatureCollection(
            feature_descriptors=[
                deepcopy(unique_fd)
                for unique_fd in {
                    fd
                    for (uuid_str, fd) in fd_subset
                    if uuid_str not in seen_uuids and not seen_uuids.add(uuid_str)
                }
            ]
        )

    @staticmethod
    def _ws_to_str(window_or_stride: Any) -&gt; str:
        &#34;&#34;&#34;Convert the window/stride value to a (shortend) string representation.&#34;&#34;&#34;
        if isinstance(window_or_stride, pd.Timedelta):
            return timedelta_to_str(window_or_stride)
        else:
            return str(window_or_stride)

    def __repr__(self) -&gt; str:
        &#34;&#34;&#34;Representation string of a FeatureCollection.&#34;&#34;&#34;
        feature_keys = sorted(set(k[0] for k in self._feature_desc_dict.keys()))
        output_str = &#34;&#34;
        for feature_key in feature_keys:
            output_str += f&#34;{&#39;|&#39;.join(feature_key)}: (&#34;
            keys = (x for x in self._feature_desc_dict.keys() if x[0] == feature_key)
            for _, win_size in keys:
                output_str += &#34;\n\twin: &#34;
                win_str = self._ws_to_str(win_size)
                output_str += f&#34;{win_str:&lt;6}: [&#34;
                for feat_desc in self._feature_desc_dict[feature_key, win_size]:
                    stride_str = feat_desc.stride
                    if stride_str is not None:
                        stride_str = [self._ws_to_str(s) for s in stride_str]
                    output_str += f&#34;\n\t\t{feat_desc._func_str}&#34;
                    output_str += f&#34;    stride: {stride_str},&#34;
                output_str += &#34;\n\t]&#34;
            output_str += &#34;\n)\n&#34;
        return output_str</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tsflex.features.feature_collection.FeatureCollection"><code class="flex name class">
<span>class <span class="ident">FeatureCollection</span></span>
<span>(</span><span>feature_descriptors=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureCollection:
    &#34;&#34;&#34;Create a FeatureCollection.

    Parameters
    ----------
    feature_descriptors : Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection, List[Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection]]], optional
        Initial (list of) feature(s) to add to collection, by default None

    Notes
    -----
    * The `series_name` property of the `FeatureDescriptor`s should **not withhold a &#34;|&#34;
      character**, since &#34;|&#34; is used to join the series names of features which use
      multiple series as input).&lt;br&gt;
      e.g.&lt;br&gt;
        * `ACC|x` is **not** allowed as series name, as this is ambiguous and could
          represent that this feature is constructed with a combination of the `ACC`
          and `x` signal.&lt;br&gt;
          Note that `max|feat` is allowed as feature output name.
    * Both the `series_name` and `output_name` property of the `FeatureDescriptor`s
      **should not withhold &#34;__&#34;** in its string representations. This constraint is
      mainly made for readability purposes.

    The two statements above will be asserted

    &#34;&#34;&#34;

    def __init__(
        self,
        feature_descriptors: Optional[
            Union[
                FeatureDescriptor,
                MultipleFeatureDescriptors,
                FeatureCollection,
                List[
                    Union[
                        FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection
                    ]
                ],
            ]
        ] = None,
    ):
        # The feature collection is a dict with keys of type:
        #   tuple(tuple(str), float OR pd.timedelta)
        # The outer tuple&#39;s values correspond to (series_key(s), window)
        self._feature_desc_dict: Dict[
            Tuple[Tuple[str, ...], Union[float, pd.Timedelta]], List[FeatureDescriptor]
        ] = {}

        if feature_descriptors:
            self.add(feature_descriptors)

    def get_required_series(self) -&gt; List[str]:
        &#34;&#34;&#34;Return all required series names for this feature collection.

        Return the list of series names that are required in order to calculate all the
        features (defined by the `FeatureDescriptor` objects) of this feature
        collection.

        Returns
        -------
        List[str]
            List of all the required series names.

        &#34;&#34;&#34;
        return list(
            set(flatten([fr_key[0] for fr_key in self._feature_desc_dict.keys()]))
        )

    def get_nb_output_features(self) -&gt; int:
        &#34;&#34;&#34;Return the number of output features in this feature collection.

        Returns
        -------
        int
            The number of output features in this feature collection.

        &#34;&#34;&#34;
        fd_list: Iterable[FeatureDescriptor] = flatten(self._feature_desc_dict.values())
        return sum(fd.get_nb_output_features() for fd in fd_list)

    def _get_nb_output_features_without_window(self) -&gt; int:
        &#34;&#34;&#34;Return the number of output features in this feature collection, without
        using the window as a unique identifier.

        This is relevant for when the window value(s) are overridden by passing
        `segment_start_idxs` and `segment_end_idxs` to the `calculate` method.

        Returns
        -------
        int:
            The number of output features in this feature collection without using the
            window as a unique identifier.

        &#34;&#34;&#34;
        return len(
            set(
                (series, o)
                for (series, _), fd_list in self._feature_desc_dict.items()
                for fd in fd_list
                for o in fd.function.output_names
            )
        )

    @staticmethod
    def _get_collection_key(
        feature: FeatureDescriptor,
    ) -&gt; Tuple[Tuple[str, ...], Union[pd.Timedelta, float, None]]:
        # Note: `window` property can be either a pd.Timedelta or a float or None
        # assert feature.window is not None
        return feature.series_name, feature.window

    def _check_feature_descriptors(
        self,
        skip_none: bool,
        calc_stride: Optional[Union[float, pd.Timedelta, None]] = None,
    ):
        &#34;&#34;&#34;Verify whether all added FeatureDescriptors imply the same-input data type.

        If this condition is not met, a warning will be raised.

        Parameters
        ----------
        skip_none: bool
            Whether to include None stride values in the checks.
        calc_stride: Union[float, pd.Timedelta, None], optional
            The `FeatureCollection.calculate` its stride argument, by default None.
            This stride takes precedence over a `FeatureDescriptor` its stride when
            it is not None.

        &#34;&#34;&#34;
        dtype_set = set()
        for series_names, win in self._feature_desc_dict.keys():
            for fd in self._feature_desc_dict[(series_names, win)]:
                stride = calc_stride if calc_stride is not None else fd.stride
                if skip_none and stride is None:
                    dtype_set.add(AttributeParser.determine_type(win))
                else:
                    dtype_set.add(
                        AttributeParser.determine_type([win] + to_list(stride))
                    )

        if len(dtype_set) &gt; 1:
            warnings.warn(
                &#34;There are multiple FeatureDescriptor window-stride &#34;
                + f&#34;datatypes present in this FeatureCollection, i.e.: {dtype_set}&#34;,
                category=RuntimeWarning,
            )

    def _add_feature(self, feature: FeatureDescriptor):
        &#34;&#34;&#34;Add a `FeatureDescriptor` instance to the collection.

        Parameters
        ----------
        feature : FeatureDescriptor
            The feature that will be added to this feature collection.

        &#34;&#34;&#34;
        # Check whether the `|` is not present in the series
        assert not any(&#34;|&#34; in s_name for s_name in feature.get_required_series())
        # Check whether the &#39;__&#34; is not present in the series and function output names
        assert not any(
            &#34;__&#34; in output_name for output_name in feature.function.output_names
        )
        assert not any(&#34;__&#34; in s_name for s_name in feature.get_required_series())

        series_win_stride_key = self._get_collection_key(feature)
        if series_win_stride_key in self._feature_desc_dict.keys():
            added_output_names = flatten(
                f.function.output_names
                for f in self._feature_desc_dict[series_win_stride_key]
            )
            # Check that not a feature with the same output_name(s) is already added
            # for the series_win_stride_key
            assert not any(
                output_name in added_output_names
                for output_name in feature.function.output_names
            )
            self._feature_desc_dict[series_win_stride_key].append(feature)
        else:
            self._feature_desc_dict[series_win_stride_key] = [feature]

    def add(
        self,
        features: Union[
            FeatureDescriptor,
            MultipleFeatureDescriptors,
            FeatureCollection,
            List[
                Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection]
            ],
        ],
    ):
        &#34;&#34;&#34;Add feature(s) to the FeatureCollection.

        Parameters
        ----------
        features : Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection, List[Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection]]]
            Feature(s) (containers) whose contained features will be added.

        Raises
        ------
        TypeError
            Raised when an item within `features` is not an instance of
            [`MultipleFeatureDescriptors`, `FeatureDescriptors`, `FeatureCollection`].

        &#34;&#34;&#34;
        # Convert to list if necessary
        features = to_list(features)

        for feature in features:
            if isinstance(feature, MultipleFeatureDescriptors):
                self.add(feature.feature_descriptions)
            elif isinstance(feature, FeatureDescriptor):
                self._add_feature(feature)
            elif isinstance(feature, FeatureCollection):
                # List needs to be flattened
                self.add(list(flatten(feature._feature_desc_dict.values())))
            else:
                raise TypeError(f&#34;type: {type(feature)} is not supported - {feature}&#34;)

        # After adding the features, check whether the descriptors are compatible
        self._check_feature_descriptors(skip_none=True)

    @staticmethod
    def _executor(idx: int):
        # global get_stroll_func
        stroll, function = get_stroll_func(idx)
        return stroll.apply_func(function)

    # def _get_stroll(self, kwargs):
    #     return StridedRollingFactory.get_segmenter(**kwargs)

    def _stroll_feat_generator(
        self,
        series_dict: Dict[str, pd.Series],
        calc_stride: Union[List[Union[float, pd.Timedelta]], None],
        segment_start_idxs: Union[np.ndarray, None],
        segment_end_idxs: Union[np.ndarray, None],
        start_idx: Any,
        end_idx: Any,
        window_idx: str,
        include_final_window: bool,
        approve_sparsity: bool,
    ) -&gt; Callable[[int], Tuple[StridedRolling, FuncWrapper]]:
        # --- Future work ---
        # We could also make the StridedRolling creation multithreaded
        # Very low priority because the STROLL __init__ is rather efficient!
        keys_wins_strides = list(self._feature_desc_dict.keys())
        lengths = np.cumsum(
            [len(self._feature_desc_dict[k]) for k in keys_wins_strides]
        )

        def get_stroll_function(idx) -&gt; Tuple[StridedRolling, FuncWrapper]:
            key_idx = np.searchsorted(lengths, idx, &#34;right&#34;)  # right bc idx starts at 0
            key, win = keys_wins_strides[key_idx]

            feature = self._feature_desc_dict[keys_wins_strides[key_idx]][
                idx - lengths[key_idx]
            ]
            stride = feature.stride if calc_stride is None else calc_stride
            function: FuncWrapper = feature.function
            # The factory method will instantiate the right StridedRolling object
            stroll_arg_dict = dict(
                data=[series_dict[k] for k in key],
                window=win,
                strides=stride,
                segment_start_idxs=segment_start_idxs,
                segment_end_idxs=segment_end_idxs,
                start_idx=start_idx,
                end_idx=end_idx,
                window_idx=window_idx,
                include_final_window=include_final_window,
                approve_sparsity=approve_sparsity,
                func_data_type=function.input_type,
            )
            stroll = StridedRollingFactory.get_segmenter(**stroll_arg_dict)
            return stroll, function

        return get_stroll_function

    def _get_stroll_feat_length(self) -&gt; int:
        return sum(
            len(self._feature_desc_dict[k]) for k in self._feature_desc_dict.keys()
        )

    def _check_no_multiple_windows(self):
        assert (
            self._get_nb_output_features_without_window()
            == self.get_nb_output_features()
        ), (
            &#34;When using `segment_XXX_idxs`; each output name - series_input combination&#34;
            + &#34; can only have 1 window (or None)&#34;
        )

    @staticmethod
    def _process_segment_idxs(
        segment_idxs: Union[list, np.ndarray, pd.Series, pd.Index]
    ) -&gt; np.ndarray:
        if hasattr(segment_idxs, &#34;values&#34;):
            segment_idxs = segment_idxs.values
        segment_idxs = np.asarray(segment_idxs)
        if segment_idxs.ndim &gt; 1:
            segment_idxs = segment_idxs.squeeze()  # remove singleton dimensions
        return segment_idxs

    def calculate(
        self,
        data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],
        stride: Optional[Union[float, str, pd.Timedelta, List, None]] = None,
        segment_start_idxs: Optional[
            Union[list, np.ndarray, pd.Series, pd.Index]
        ] = None,
        segment_end_idxs: Optional[Union[list, np.ndarray, pd.Series, pd.Index]] = None,
        return_df: Optional[bool] = False,
        window_idx: Optional[str] = &#34;end&#34;,
        include_final_window: Optional[bool] = False,
        bound_method: Optional[str] = &#34;inner&#34;,
        approve_sparsity: Optional[bool] = False,
        show_progress: Optional[bool] = False,
        logging_file_path: Optional[Union[str, Path]] = None,
        n_jobs: Optional[int] = None,
    ) -&gt; Union[List[pd.DataFrame], pd.DataFrame]:
        &#34;&#34;&#34;Calculate features on the passed data.

        Parameters
        ----------
        data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]
            Dataframe or Series or list thereof, with all the required data for the
            feature calculation. \n
            **Assumptions**: \n
            * each Series / DataFrame must have a sortable index. This index represents
            the sequence position of the corresponding values, the index can be either
            numeric or a ``pd.DatetimeIndex``.
            * each Series / DataFrame index must be comparable with all others
            * we assume that each series-name / dataframe-column-name is unique.
        stride: Union[float, str, pd.Timedelta, List[Union[float, str, pd.Timedelta], None], optional
            The stride size. By default None. This argument supports multiple types: \n
            * If None, the stride of the `FeatureDescriptor` objects will be used.
            * If the type is an `float` or an `int`, its value represents the series:\n
                - its stride **range** when a **non time-indexed** series is passed.
                - the stride in **number of samples**, when a **time-indexed** series
                is passed (must then be and `int`)
            * If the stride&#39;s type is a `pd.Timedelta`, the stride size represents
            the stride-time delta. The passed data **must have a time-index**.
            * If a `str`, it must represent a stride-time-delta-string. Hence, the
            **passed data must have a time-index**. \n
            .. Note::
                When set, this stride argument takes precedence over the stride property
                of the `FeatureDescriptor`s in this `FeatureCollection` (i.e., when a
                not None value for `stride` passed to this method).
        segment_start_idxs: Union[list, np.ndarray, pd.Series, pd.Index], optional
            The start indices of the segments. If None, the start indices will be
            computed from the data using either:\n
            - the `segment_end_idxs` - the `window` size property of the
                `FeatureDescriptor` in this `FeatureCollection` (if `segment_end_idxs`
                is not None)
            - strided-window rolling on the data using `window` and `stride` of the
                `FeatureDescriptor` in this `FeatureCollection` (if `segment_end_idxs`
                 is also None). (Note that the `stride` argument of this method takes
                 precedence over the `stride` property of the `FeatureDescriptor`s).
            By default None.
        segment_end_idxs: Union[list, np.ndarray, pd.Series, pd.Index], optional
            The end indices for the segmented windows. If None, the end indices will be
            computed from the data using either:\n
            - the `segment_start_idxs` + the `window` size property of the
                `FeatureDescriptor` in this `FeatureCollection` (if `segment_start_idxs`
                is not None)
            - strided-window rolling on the data using `window` and `stride` of the
                `FeatureDescriptor` in this `FeatureCollection` (if `segment_start_idxs`
                 is also None). (Note that the `stride` argument of this method takes
                 precedence over the `stride` property of the `FeatureDescriptor`s).
            By default None.

            ..Note::
                When passing both `segment_start_idxs` and `segment_end_idxs`, these two
                arguments must have the same length and every start index must be &lt;=
                than the corresponding end index.
                Note that passing both arguments, discards any meaning of the `window`
                and `stride` values (as these segment indices define the segmented data,
                and thus no strided-window rolling index calculation has to be executed).
                As such, the user can create variable-length segmented windows. However,
                in such cases, the user should be weary that the feature functions are
                invariant to these (potentially variable-length) windows.
        return_df : bool, optional
            Whether the output needs to be a DataFrame or a list thereof, by default
            False. If `True` the output dataframes will be merged to a DataFrame with an
            outer merge.
        window_idx : str, optional
            The window&#39;s index position which will be used as index for the
            feature_window aggregation. Must be either of: `[&#34;begin&#34;, &#34;middle&#34;, &#34;end&#34;]`.
            by **default &#34;end&#34;**. All features in this collection will use the same
            window_idx.

            ..Note::
                `window_idx`=&#34;end&#34; uses the window&#39;s end (= right open bound) as
                output index. \n
                `window_idx`=&#34;begin&#34; uses the window&#39;s start idx (= left closed bound)
                as output index.
        include_final_window : bool, optional
            Whether the final (possibly incomplete) window should be included in the
            strided-window segmentation, by default False.

            .. Note::
                The remarks below apply when `include_final_window` is set to True.
                The user should be aware that the last window *might* be incomplete,
                i.e.;

                - when equally sampled, the last window *might* be smaller than the
                  the other windows.
                - when not equally sampled, the last window *might* not include all the
                  data points (as the begin-time + window-size comes after the last data
                  point).

                Note, that when equally sampled, the last window *will* be a full window
                when:

                - the stride is the sampling rate of the data (or stride = 1 for
                sample-based configurations).&lt;br&gt;
                **Remark**: that when `include_final_window` is set to False, the last
                window (which is a full) window will not be included!
                - *(len * sampling_rate - window_size) % stride = 0*. Remark that the
                  above case is a base case of this.
        bound_method: str, optional
            The start-end bound methodology which is used to generate the slice ranges
            when ``data`` consists of multiple series / columns.
            Must be either of: `[&#34;inner&#34;, &#34;inner-outer&#34;, &#34;outer&#34;]`, by default &#34;inner&#34;.

            * if ``inner``, the inner-bounds of the series are returned.
            * if ``inner-outer``, the left-inner and right-outer bounds of the series
              are returned.
            * if ``outer``, the outer-bounds of the series are returned.
        approve_sparsity: bool, optional
            Bool indicating whether the user acknowledges that there may be sparsity
            (i.e., irregularly sampled data), by default False.
            If False and sparsity is observed, a warning is raised.
        show_progress: bool, optional
            If True, the progress will be shown with a progressbar, by default False.
        logging_file_path : Union[str, Path], optional
            The file path where the logged messages are stored. If `None`, then no
            logging `FileHandler` will be used and the logging messages are only pushed
            to stdout. Otherwise, a logging `FileHandler` will write the logged messages
            to the given file path. See also the `tsflex.features.logger` module.
        n_jobs : int, optional
            The number of processes used for the feature calculation. If `None`, then
            the number returned by _os.cpu_count()_ is used, by default None. \n
            If n_jobs is either 0 or 1, the code will be executed sequentially without
            creating a process pool. This is very useful when debugging, as the stack
            trace will be more comprehensible.
            .. note::
                Multiprocessed execution is not supported on Windows. Even when,
                `n_jobs` is set &gt; 1, the feature extraction will still be executed
                sequentially.
                Why do we not support multiprocessing on Windows; see this issue
                https://github.com/predict-idlab/tsflex/issues/51

            .. tip::
                It takes on avg. _300ms_ to schedule everything with
                multiprocessing. So if your sequential feature extraction code runs
                faster than ~1s, it might not be worth it to parallelize the process
                (and thus better leave `n_jobs` to 0 or 1).

        Returns
        -------
        Union[List[pd.DataFrame], pd.DataFrame]
            The calculated features.

        Raises
        ------
        KeyError
            Raised when a required key is not found in `data`.

        Notes
        ------
        * The (column-)names of the series in `data` represent the `series_names`.
        * If a `logging_file_path` is provided, the execution (time) info can be
          retrieved by calling `logger.get_feature_logs(logging_file_path)`.
          Be aware that the `logging_file_path` gets cleared before the logger pushes
          logged messages. Hence, one should use a separate logging file for each
          constructed processing and feature instance with this library.


        &#34;&#34;&#34;
        # Delete other logging handlers
        delete_logging_handlers(logger)
        # Add logging handler (if path provided)
        if logging_file_path:
            f_handler = add_logging_handler(logger, logging_file_path)

        # Convert to numpy array (if necessary)
        if segment_start_idxs is not None:
            segment_start_idxs = FeatureCollection._process_segment_idxs(
                segment_start_idxs
            )
        if segment_end_idxs is not None:
            segment_end_idxs = FeatureCollection._process_segment_idxs(segment_end_idxs)

        if segment_start_idxs is not None and segment_end_idxs is not None:
            # Check if segment indices have same length and whether every start idx
            # &lt;= end idx
            _check_start_end_array(segment_start_idxs, segment_end_idxs)
            # Check if there is either 1 or No(ne) window value for every output name -
            # input_series combination
            self._check_no_multiple_windows()

        if segment_start_idxs is None or segment_end_idxs is None:
            assert all(
                fd.window is not None
                for fd in flatten(self._feature_desc_dict.values())
            ), (
                &#34;Each feature descriptor must have a window when not both &#34;
                + &#34;segment_start_idxs and segment_end_idxs are provided&#34;
            )

        if stride is None and segment_start_idxs is None and segment_end_idxs is None:
            assert all(
                fd.stride is not None
                for fd in flatten(self._feature_desc_dict.values())
            ), (
                &#34;Each feature descriptor must have a stride when no stride or &#34;
                + &#34;segment indices are passed to this method!&#34;
            )
        elif stride is not None and (
            segment_start_idxs is not None or segment_end_idxs is not None
        ):
            raise ValueError(
                &#34;The stride and any segment index argument cannot be set together! &#34;
                + &#34;At least one of both should be None.&#34;
            )

        if stride is not None:
            # Verify whether the stride complies with the input data dtype
            stride = [
                parse_time_arg(s) if isinstance(s, str) else s for s in to_list(stride)
            ]
            self._check_feature_descriptors(skip_none=False, calc_stride=stride)

        # Convert the data to a series_dict
        series_dict: Dict[str, pd.Series] = {}
        for s in to_series_list(data):
            if not s.index.is_monotonic_increasing:
                warnings.warn(
                    f&#34;The index of series &#39;{s.name}&#39; is not monotonic increasing. &#34;
                    + &#34;The series will be sorted by the index.&#34;,
                    RuntimeWarning,
                )
                s = s.sort_index(ascending=True, inplace=False, ignore_index=False)

            # Assert the assumptions we make!
            assert s.index.is_monotonic_increasing

            if s.name in self.get_required_series():
                series_dict[str(s.name)] = s

        # Determine the bounds of the series dict items and slice on them
        # TODO: is dit wel nodig `hier? want we doen dat ook in de strided rolling
        start, end = _determine_bounds(bound_method, list(series_dict.values()))
        series_dict = {
            n: s.loc[
                s.index.dtype.type(start) : s.index.dtype.type(end)
            ]  # TODO: check memory efficiency of ths
            for n, s, in series_dict.items()
        }

        # Note: this variable has a global scope so this is shared in multiprocessing
        # TODO: try to make this more efficient (but is not really the bottleneck)
        global get_stroll_func
        get_stroll_func = self._stroll_feat_generator(
            series_dict,
            calc_stride=stride,
            segment_start_idxs=segment_start_idxs,
            segment_end_idxs=segment_end_idxs,
            start_idx=start,
            end_idx=end,
            window_idx=window_idx,
            include_final_window=include_final_window,
            approve_sparsity=approve_sparsity,
        )
        nb_stroll_funcs = self._get_stroll_feat_length()

        if (
            os.name == &#34;nt&#34;
        ):  # On Windows no multiprocessing is supported, see https://github.com/predict-idlab/tsflex/issues/51
            n_jobs = 1
        elif n_jobs is None:
            n_jobs = os.cpu_count()
        n_jobs = min(n_jobs, nb_stroll_funcs)

        calculated_feature_list = None
        if n_jobs in [0, 1]:
            idxs = range(nb_stroll_funcs)
            if show_progress:
                idxs = tqdm(idxs)
            try:
                calculated_feature_list = [self._executor(idx) for idx in idxs]
            except Exception:
                traceback.print_exc()
        else:
            with Pool(processes=n_jobs) as pool:
                results = pool.imap_unordered(self._executor, range(nb_stroll_funcs))
                if show_progress:
                    results = tqdm(results, total=nb_stroll_funcs)
                try:
                    calculated_feature_list = [f for f in results]
                except Exception:
                    traceback.print_exc()
                    pool.terminate()
                finally:
                    # Close &amp; join because: https://github.com/uqfoundation/pathos/issues/131
                    pool.close()
                    pool.join()

        # Close the file handler (this avoids PermissionError: [WinError 32])
        if logging_file_path:
            f_handler.close()
            logger.removeHandler(f_handler)

        if calculated_feature_list is None:
            raise RuntimeError(
                &#34;Feature Extraction halted due to error while extracting one &#34;
                + &#34;(or multiple) feature(s)! See stack trace above.&#34;
            )

        if return_df:
            # concatenate &amp; sort the columns
            df = pd.concat(calculated_feature_list, axis=1, join=&#34;outer&#34;, copy=False)
            return df.reindex(sorted(df.columns), axis=1)
        else:
            return calculated_feature_list

    def serialize(self, file_path: Union[str, Path]):
        &#34;&#34;&#34;Serialize this FeatureCollection instance.

        Parameters
        ----------
        file_path : Union[str, Path]
            The path where the `FeatureCollection` will be serialized.

        Note
        -----
        As we use [Dill](https://github.com/uqfoundation/dill){:target=&#34;_blank&#34;} to
        serialize the files, we can **also serialize functions which are defined in
        the local scope, like lambdas.**

        &#34;&#34;&#34;
        with open(file_path, &#34;wb&#34;) as f:
            dill.dump(self, f, recurse=True)

    def reduce(self, feat_cols_to_keep: List[str]) -&gt; FeatureCollection:
        &#34;&#34;&#34;Create a reduced FeatureCollection instance based on `feat_cols_to_keep`.

        For example, this is useful to optimize feature-extraction inference
        (for your selected features) after performing a feature-selection procedure.

        Parameters
        ----------
        feat_cols_to_keep: List[str]
            A subset of the feature collection instance its column names.
            This corresponds to the columns / names of the output from `calculate`
            method that you want to keep.

        Returns
        -------
        FeatureCollection
            A new FeatureCollection object, which only withholds the FeatureDescriptors
            which constitute the `feat_cols_to_keep` output.

        Note
        ----
        Some FeatureDescriptor objects may have multiple **output-names**.&lt;br&gt;
        Hence, if you only want to retain _a subset_ of that FeatureDescriptor its
        feature outputs, you will still get **all features** as the new
        FeatureCollection is constructed by applying a filter on de FeatureDescriptor
        list and we thus not alter these FeatureDescriptor objects themselves.

        &#34;&#34;&#34;
        # dict in which we store all the { output_col_name : (UUID, FeatureDescriptor) }
        # items of our current FeatureCollection object
        manual_window = False
        if any(c.endswith(&#34;w=manual&#34;) for c in feat_cols_to_keep):
            assert all(c.endswith(&#34;w=manual&#34;) for c in feat_cols_to_keep)
            # As the windows are created manual, the FeatureCollection cannot contain
            # multiple windows for the same output name - input_series combination
            self._check_no_multiple_windows()
            manual_window = True
        feat_col_fd_mapping: Dict[str, Tuple[str, FeatureDescriptor]] = {}
        for (s_names, window), fd_list in self._feature_desc_dict.items():
            window = &#34;manual&#34; if manual_window else self._ws_to_str(window)
            for fd in fd_list:
                # As a single FeatureDescriptor can have multiple output col names, we
                # create a unique identifier for each FeatureDescriptor (on which we
                # will apply set-like operations later on to only retain all the unique
                # FeatureDescriptors)
                uuid_str = str(uuid.uuid4())
                for output_name in fd.function.output_names:
                    # Reconstruct the feature column name
                    feat_col_name = StridedRolling.construct_output_index(
                        series_keys=s_names, feat_name=output_name, win_str=window
                    )
                    feat_col_fd_mapping[feat_col_name] = (uuid_str, fd)

        assert all(fc in feat_col_fd_mapping for fc in feat_cols_to_keep)

        # Collect (uuid, FeatureDescriptor) for the feat_cols_to_keep
        fd_subset: List[Tuple[str, FeatureDescriptor]] = [
            feat_col_fd_mapping[fc] for fc in feat_cols_to_keep
        ]

        # Reduce to unique feature descriptor objects (based on uuid) and create a new
        # FeatureCollection for their deepcopy&#39;s.
        seen_uuids = set()
        return FeatureCollection(
            feature_descriptors=[
                deepcopy(unique_fd)
                for unique_fd in {
                    fd
                    for (uuid_str, fd) in fd_subset
                    if uuid_str not in seen_uuids and not seen_uuids.add(uuid_str)
                }
            ]
        )

    @staticmethod
    def _ws_to_str(window_or_stride: Any) -&gt; str:
        &#34;&#34;&#34;Convert the window/stride value to a (shortend) string representation.&#34;&#34;&#34;
        if isinstance(window_or_stride, pd.Timedelta):
            return timedelta_to_str(window_or_stride)
        else:
            return str(window_or_stride)

    def __repr__(self) -&gt; str:
        &#34;&#34;&#34;Representation string of a FeatureCollection.&#34;&#34;&#34;
        feature_keys = sorted(set(k[0] for k in self._feature_desc_dict.keys()))
        output_str = &#34;&#34;
        for feature_key in feature_keys:
            output_str += f&#34;{&#39;|&#39;.join(feature_key)}: (&#34;
            keys = (x for x in self._feature_desc_dict.keys() if x[0] == feature_key)
            for _, win_size in keys:
                output_str += &#34;\n\twin: &#34;
                win_str = self._ws_to_str(win_size)
                output_str += f&#34;{win_str:&lt;6}: [&#34;
                for feat_desc in self._feature_desc_dict[feature_key, win_size]:
                    stride_str = feat_desc.stride
                    if stride_str is not None:
                        stride_str = [self._ws_to_str(s) for s in stride_str]
                    output_str += f&#34;\n\t\t{feat_desc._func_str}&#34;
                    output_str += f&#34;    stride: {stride_str},&#34;
                output_str += &#34;\n\t]&#34;
            output_str += &#34;\n)\n&#34;
        return output_str</code></pre>
</details>
<div class="desc"><p>Create a FeatureCollection.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>feature_descriptors</code></strong> :&ensp;<code>Union[FeatureDescriptor, MultipleFeatureDescriptors, <a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a>, List[Union[FeatureDescriptor, MultipleFeatureDescriptors, <a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a>]]]</code>, optional</dt>
<dd>Initial (list of) feature(s) to add to collection, by default None</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li>The <code>series_name</code> property of the <code>FeatureDescriptor</code>s should <strong>not withhold a "|"
character</strong>, since "|" is used to join the series names of features which use
multiple series as input).<br>
e.g.<br><ul>
<li><code>ACC|x</code> is <strong>not</strong> allowed as series name, as this is ambiguous and could
represent that this feature is constructed with a combination of the <code>ACC</code>
and <code>x</code> signal.<br>
Note that <code>max|feat</code> is allowed as feature output name.</li>
</ul>
</li>
<li>Both the <code>series_name</code> and <code>output_name</code> property of the <code>FeatureDescriptor</code>s
<strong>should not withhold "__"</strong> in its string representations. This constraint is
mainly made for readability purposes.</li>
</ul>
<p>The two statements above will be asserted</p></div>
<h3>Methods</h3>
<dl>
<dt id="tsflex.features.feature_collection.FeatureCollection.get_required_series"><code class="name flex">
<span>def <span class="ident">get_required_series</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_required_series(self) -&gt; List[str]:
    &#34;&#34;&#34;Return all required series names for this feature collection.

    Return the list of series names that are required in order to calculate all the
    features (defined by the `FeatureDescriptor` objects) of this feature
    collection.

    Returns
    -------
    List[str]
        List of all the required series names.

    &#34;&#34;&#34;
    return list(
        set(flatten([fr_key[0] for fr_key in self._feature_desc_dict.keys()]))
    )</code></pre>
</details>
<div class="desc"><p>Return all required series names for this feature collection.</p>
<p>Return the list of series names that are required in order to calculate all the
features (defined by the <code>FeatureDescriptor</code> objects) of this feature
collection.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[str]</code></dt>
<dd>List of all the required series names.</dd>
</dl></div>
</dd>
<dt id="tsflex.features.feature_collection.FeatureCollection.get_nb_output_features"><code class="name flex">
<span>def <span class="ident">get_nb_output_features</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_nb_output_features(self) -&gt; int:
    &#34;&#34;&#34;Return the number of output features in this feature collection.

    Returns
    -------
    int
        The number of output features in this feature collection.

    &#34;&#34;&#34;
    fd_list: Iterable[FeatureDescriptor] = flatten(self._feature_desc_dict.values())
    return sum(fd.get_nb_output_features() for fd in fd_list)</code></pre>
</details>
<div class="desc"><p>Return the number of output features in this feature collection.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>The number of output features in this feature collection.</dd>
</dl></div>
</dd>
<dt id="tsflex.features.feature_collection.FeatureCollection.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self, features)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add(
    self,
    features: Union[
        FeatureDescriptor,
        MultipleFeatureDescriptors,
        FeatureCollection,
        List[
            Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection]
        ],
    ],
):
    &#34;&#34;&#34;Add feature(s) to the FeatureCollection.

    Parameters
    ----------
    features : Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection, List[Union[FeatureDescriptor, MultipleFeatureDescriptors, FeatureCollection]]]
        Feature(s) (containers) whose contained features will be added.

    Raises
    ------
    TypeError
        Raised when an item within `features` is not an instance of
        [`MultipleFeatureDescriptors`, `FeatureDescriptors`, `FeatureCollection`].

    &#34;&#34;&#34;
    # Convert to list if necessary
    features = to_list(features)

    for feature in features:
        if isinstance(feature, MultipleFeatureDescriptors):
            self.add(feature.feature_descriptions)
        elif isinstance(feature, FeatureDescriptor):
            self._add_feature(feature)
        elif isinstance(feature, FeatureCollection):
            # List needs to be flattened
            self.add(list(flatten(feature._feature_desc_dict.values())))
        else:
            raise TypeError(f&#34;type: {type(feature)} is not supported - {feature}&#34;)

    # After adding the features, check whether the descriptors are compatible
    self._check_feature_descriptors(skip_none=True)</code></pre>
</details>
<div class="desc"><p>Add feature(s) to the FeatureCollection.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>features</code></strong> :&ensp;<code>Union[FeatureDescriptor, MultipleFeatureDescriptors, <a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a>, List[Union[FeatureDescriptor, MultipleFeatureDescriptors, <a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a>]]]</code></dt>
<dd>Feature(s) (containers) whose contained features will be added.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>TypeError</code></dt>
<dd>Raised when an item within <code>features</code> is not an instance of
[<code>MultipleFeatureDescriptors</code>, <code>FeatureDescriptors</code>, <code><a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a></code>].</dd>
</dl></div>
</dd>
<dt id="tsflex.features.feature_collection.FeatureCollection.calculate"><code class="name flex">
<span>def <span class="ident">calculate</span></span>(<span>self, data, stride=None, segment_start_idxs=None, segment_end_idxs=None, return_df=False, window_idx='end', include_final_window=False, bound_method='inner', approve_sparsity=False, show_progress=False, logging_file_path=None, n_jobs=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate(
    self,
    data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],
    stride: Optional[Union[float, str, pd.Timedelta, List, None]] = None,
    segment_start_idxs: Optional[
        Union[list, np.ndarray, pd.Series, pd.Index]
    ] = None,
    segment_end_idxs: Optional[Union[list, np.ndarray, pd.Series, pd.Index]] = None,
    return_df: Optional[bool] = False,
    window_idx: Optional[str] = &#34;end&#34;,
    include_final_window: Optional[bool] = False,
    bound_method: Optional[str] = &#34;inner&#34;,
    approve_sparsity: Optional[bool] = False,
    show_progress: Optional[bool] = False,
    logging_file_path: Optional[Union[str, Path]] = None,
    n_jobs: Optional[int] = None,
) -&gt; Union[List[pd.DataFrame], pd.DataFrame]:
    &#34;&#34;&#34;Calculate features on the passed data.

    Parameters
    ----------
    data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]
        Dataframe or Series or list thereof, with all the required data for the
        feature calculation. \n
        **Assumptions**: \n
        * each Series / DataFrame must have a sortable index. This index represents
        the sequence position of the corresponding values, the index can be either
        numeric or a ``pd.DatetimeIndex``.
        * each Series / DataFrame index must be comparable with all others
        * we assume that each series-name / dataframe-column-name is unique.
    stride: Union[float, str, pd.Timedelta, List[Union[float, str, pd.Timedelta], None], optional
        The stride size. By default None. This argument supports multiple types: \n
        * If None, the stride of the `FeatureDescriptor` objects will be used.
        * If the type is an `float` or an `int`, its value represents the series:\n
            - its stride **range** when a **non time-indexed** series is passed.
            - the stride in **number of samples**, when a **time-indexed** series
            is passed (must then be and `int`)
        * If the stride&#39;s type is a `pd.Timedelta`, the stride size represents
        the stride-time delta. The passed data **must have a time-index**.
        * If a `str`, it must represent a stride-time-delta-string. Hence, the
        **passed data must have a time-index**. \n
        .. Note::
            When set, this stride argument takes precedence over the stride property
            of the `FeatureDescriptor`s in this `FeatureCollection` (i.e., when a
            not None value for `stride` passed to this method).
    segment_start_idxs: Union[list, np.ndarray, pd.Series, pd.Index], optional
        The start indices of the segments. If None, the start indices will be
        computed from the data using either:\n
        - the `segment_end_idxs` - the `window` size property of the
            `FeatureDescriptor` in this `FeatureCollection` (if `segment_end_idxs`
            is not None)
        - strided-window rolling on the data using `window` and `stride` of the
            `FeatureDescriptor` in this `FeatureCollection` (if `segment_end_idxs`
             is also None). (Note that the `stride` argument of this method takes
             precedence over the `stride` property of the `FeatureDescriptor`s).
        By default None.
    segment_end_idxs: Union[list, np.ndarray, pd.Series, pd.Index], optional
        The end indices for the segmented windows. If None, the end indices will be
        computed from the data using either:\n
        - the `segment_start_idxs` + the `window` size property of the
            `FeatureDescriptor` in this `FeatureCollection` (if `segment_start_idxs`
            is not None)
        - strided-window rolling on the data using `window` and `stride` of the
            `FeatureDescriptor` in this `FeatureCollection` (if `segment_start_idxs`
             is also None). (Note that the `stride` argument of this method takes
             precedence over the `stride` property of the `FeatureDescriptor`s).
        By default None.

        ..Note::
            When passing both `segment_start_idxs` and `segment_end_idxs`, these two
            arguments must have the same length and every start index must be &lt;=
            than the corresponding end index.
            Note that passing both arguments, discards any meaning of the `window`
            and `stride` values (as these segment indices define the segmented data,
            and thus no strided-window rolling index calculation has to be executed).
            As such, the user can create variable-length segmented windows. However,
            in such cases, the user should be weary that the feature functions are
            invariant to these (potentially variable-length) windows.
    return_df : bool, optional
        Whether the output needs to be a DataFrame or a list thereof, by default
        False. If `True` the output dataframes will be merged to a DataFrame with an
        outer merge.
    window_idx : str, optional
        The window&#39;s index position which will be used as index for the
        feature_window aggregation. Must be either of: `[&#34;begin&#34;, &#34;middle&#34;, &#34;end&#34;]`.
        by **default &#34;end&#34;**. All features in this collection will use the same
        window_idx.

        ..Note::
            `window_idx`=&#34;end&#34; uses the window&#39;s end (= right open bound) as
            output index. \n
            `window_idx`=&#34;begin&#34; uses the window&#39;s start idx (= left closed bound)
            as output index.
    include_final_window : bool, optional
        Whether the final (possibly incomplete) window should be included in the
        strided-window segmentation, by default False.

        .. Note::
            The remarks below apply when `include_final_window` is set to True.
            The user should be aware that the last window *might* be incomplete,
            i.e.;

            - when equally sampled, the last window *might* be smaller than the
              the other windows.
            - when not equally sampled, the last window *might* not include all the
              data points (as the begin-time + window-size comes after the last data
              point).

            Note, that when equally sampled, the last window *will* be a full window
            when:

            - the stride is the sampling rate of the data (or stride = 1 for
            sample-based configurations).&lt;br&gt;
            **Remark**: that when `include_final_window` is set to False, the last
            window (which is a full) window will not be included!
            - *(len * sampling_rate - window_size) % stride = 0*. Remark that the
              above case is a base case of this.
    bound_method: str, optional
        The start-end bound methodology which is used to generate the slice ranges
        when ``data`` consists of multiple series / columns.
        Must be either of: `[&#34;inner&#34;, &#34;inner-outer&#34;, &#34;outer&#34;]`, by default &#34;inner&#34;.

        * if ``inner``, the inner-bounds of the series are returned.
        * if ``inner-outer``, the left-inner and right-outer bounds of the series
          are returned.
        * if ``outer``, the outer-bounds of the series are returned.
    approve_sparsity: bool, optional
        Bool indicating whether the user acknowledges that there may be sparsity
        (i.e., irregularly sampled data), by default False.
        If False and sparsity is observed, a warning is raised.
    show_progress: bool, optional
        If True, the progress will be shown with a progressbar, by default False.
    logging_file_path : Union[str, Path], optional
        The file path where the logged messages are stored. If `None`, then no
        logging `FileHandler` will be used and the logging messages are only pushed
        to stdout. Otherwise, a logging `FileHandler` will write the logged messages
        to the given file path. See also the `tsflex.features.logger` module.
    n_jobs : int, optional
        The number of processes used for the feature calculation. If `None`, then
        the number returned by _os.cpu_count()_ is used, by default None. \n
        If n_jobs is either 0 or 1, the code will be executed sequentially without
        creating a process pool. This is very useful when debugging, as the stack
        trace will be more comprehensible.
        .. note::
            Multiprocessed execution is not supported on Windows. Even when,
            `n_jobs` is set &gt; 1, the feature extraction will still be executed
            sequentially.
            Why do we not support multiprocessing on Windows; see this issue
            https://github.com/predict-idlab/tsflex/issues/51

        .. tip::
            It takes on avg. _300ms_ to schedule everything with
            multiprocessing. So if your sequential feature extraction code runs
            faster than ~1s, it might not be worth it to parallelize the process
            (and thus better leave `n_jobs` to 0 or 1).

    Returns
    -------
    Union[List[pd.DataFrame], pd.DataFrame]
        The calculated features.

    Raises
    ------
    KeyError
        Raised when a required key is not found in `data`.

    Notes
    ------
    * The (column-)names of the series in `data` represent the `series_names`.
    * If a `logging_file_path` is provided, the execution (time) info can be
      retrieved by calling `logger.get_feature_logs(logging_file_path)`.
      Be aware that the `logging_file_path` gets cleared before the logger pushes
      logged messages. Hence, one should use a separate logging file for each
      constructed processing and feature instance with this library.


    &#34;&#34;&#34;
    # Delete other logging handlers
    delete_logging_handlers(logger)
    # Add logging handler (if path provided)
    if logging_file_path:
        f_handler = add_logging_handler(logger, logging_file_path)

    # Convert to numpy array (if necessary)
    if segment_start_idxs is not None:
        segment_start_idxs = FeatureCollection._process_segment_idxs(
            segment_start_idxs
        )
    if segment_end_idxs is not None:
        segment_end_idxs = FeatureCollection._process_segment_idxs(segment_end_idxs)

    if segment_start_idxs is not None and segment_end_idxs is not None:
        # Check if segment indices have same length and whether every start idx
        # &lt;= end idx
        _check_start_end_array(segment_start_idxs, segment_end_idxs)
        # Check if there is either 1 or No(ne) window value for every output name -
        # input_series combination
        self._check_no_multiple_windows()

    if segment_start_idxs is None or segment_end_idxs is None:
        assert all(
            fd.window is not None
            for fd in flatten(self._feature_desc_dict.values())
        ), (
            &#34;Each feature descriptor must have a window when not both &#34;
            + &#34;segment_start_idxs and segment_end_idxs are provided&#34;
        )

    if stride is None and segment_start_idxs is None and segment_end_idxs is None:
        assert all(
            fd.stride is not None
            for fd in flatten(self._feature_desc_dict.values())
        ), (
            &#34;Each feature descriptor must have a stride when no stride or &#34;
            + &#34;segment indices are passed to this method!&#34;
        )
    elif stride is not None and (
        segment_start_idxs is not None or segment_end_idxs is not None
    ):
        raise ValueError(
            &#34;The stride and any segment index argument cannot be set together! &#34;
            + &#34;At least one of both should be None.&#34;
        )

    if stride is not None:
        # Verify whether the stride complies with the input data dtype
        stride = [
            parse_time_arg(s) if isinstance(s, str) else s for s in to_list(stride)
        ]
        self._check_feature_descriptors(skip_none=False, calc_stride=stride)

    # Convert the data to a series_dict
    series_dict: Dict[str, pd.Series] = {}
    for s in to_series_list(data):
        if not s.index.is_monotonic_increasing:
            warnings.warn(
                f&#34;The index of series &#39;{s.name}&#39; is not monotonic increasing. &#34;
                + &#34;The series will be sorted by the index.&#34;,
                RuntimeWarning,
            )
            s = s.sort_index(ascending=True, inplace=False, ignore_index=False)

        # Assert the assumptions we make!
        assert s.index.is_monotonic_increasing

        if s.name in self.get_required_series():
            series_dict[str(s.name)] = s

    # Determine the bounds of the series dict items and slice on them
    # TODO: is dit wel nodig `hier? want we doen dat ook in de strided rolling
    start, end = _determine_bounds(bound_method, list(series_dict.values()))
    series_dict = {
        n: s.loc[
            s.index.dtype.type(start) : s.index.dtype.type(end)
        ]  # TODO: check memory efficiency of ths
        for n, s, in series_dict.items()
    }

    # Note: this variable has a global scope so this is shared in multiprocessing
    # TODO: try to make this more efficient (but is not really the bottleneck)
    global get_stroll_func
    get_stroll_func = self._stroll_feat_generator(
        series_dict,
        calc_stride=stride,
        segment_start_idxs=segment_start_idxs,
        segment_end_idxs=segment_end_idxs,
        start_idx=start,
        end_idx=end,
        window_idx=window_idx,
        include_final_window=include_final_window,
        approve_sparsity=approve_sparsity,
    )
    nb_stroll_funcs = self._get_stroll_feat_length()

    if (
        os.name == &#34;nt&#34;
    ):  # On Windows no multiprocessing is supported, see https://github.com/predict-idlab/tsflex/issues/51
        n_jobs = 1
    elif n_jobs is None:
        n_jobs = os.cpu_count()
    n_jobs = min(n_jobs, nb_stroll_funcs)

    calculated_feature_list = None
    if n_jobs in [0, 1]:
        idxs = range(nb_stroll_funcs)
        if show_progress:
            idxs = tqdm(idxs)
        try:
            calculated_feature_list = [self._executor(idx) for idx in idxs]
        except Exception:
            traceback.print_exc()
    else:
        with Pool(processes=n_jobs) as pool:
            results = pool.imap_unordered(self._executor, range(nb_stroll_funcs))
            if show_progress:
                results = tqdm(results, total=nb_stroll_funcs)
            try:
                calculated_feature_list = [f for f in results]
            except Exception:
                traceback.print_exc()
                pool.terminate()
            finally:
                # Close &amp; join because: https://github.com/uqfoundation/pathos/issues/131
                pool.close()
                pool.join()

    # Close the file handler (this avoids PermissionError: [WinError 32])
    if logging_file_path:
        f_handler.close()
        logger.removeHandler(f_handler)

    if calculated_feature_list is None:
        raise RuntimeError(
            &#34;Feature Extraction halted due to error while extracting one &#34;
            + &#34;(or multiple) feature(s)! See stack trace above.&#34;
        )

    if return_df:
        # concatenate &amp; sort the columns
        df = pd.concat(calculated_feature_list, axis=1, join=&#34;outer&#34;, copy=False)
        return df.reindex(sorted(df.columns), axis=1)
    else:
        return calculated_feature_list</code></pre>
</details>
<div class="desc"><p>Calculate features on the passed data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]</code></dt>
<dd>
<p>Dataframe or Series or list thereof, with all the required data for the
feature calculation. </p>
<p><strong>Assumptions</strong>: </p>
<ul>
<li>each Series / DataFrame must have a sortable index. This index represents
the sequence position of the corresponding values, the index can be either
numeric or a <code>pd.DatetimeIndex</code>.</li>
<li>each Series / DataFrame index must be comparable with all others</li>
<li>we assume that each series-name / dataframe-column-name is unique.</li>
</ul>
</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>Union[float, str, pd.Timedelta, List[Union[float, str, pd.Timedelta], None]</code>, optional</dt>
<dd>
<p>The stride size. By default None. This argument supports multiple types: </p>
<ul>
<li>If None, the stride of the <code>FeatureDescriptor</code> objects will be used.</li>
<li>
<p>If the type is an <code>float</code> or an <code>int</code>, its value represents the series:</p>
<ul>
<li>its stride <strong>range</strong> when a <strong>non time-indexed</strong> series is passed.</li>
<li>the stride in <strong>number of samples</strong>, when a <strong>time-indexed</strong> series
is passed (must then be and <code>int</code>)<ul>
<li>If the stride's type is a <code>pd.Timedelta</code>, the stride size represents
the stride-time delta. The passed data <strong>must have a time-index</strong>.</li>
<li>If a <code>str</code>, it must represent a stride-time-delta-string. Hence, the
<strong>passed data must have a time-index</strong>. </li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When set, this stride argument takes precedence over the stride property
of the <code>FeatureDescriptor</code>s in this <code><a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a></code> (i.e., when a
not None value for <code>stride</code> passed to this method).</p>
</div>
</dd>
<dt><strong><code>segment_start_idxs</code></strong> :&ensp;<code>Union[list, np.ndarray, pd.Series, pd.Index]</code>, optional</dt>
<dd>
<p>The start indices of the segments. If None, the start indices will be
computed from the data using either:</p>
<ul>
<li>the <code>segment_end_idxs</code> - the <code>window</code> size property of the
<code>FeatureDescriptor</code> in this <code><a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a></code> (if <code>segment_end_idxs</code>
is not None)</li>
<li>strided-window rolling on the data using <code>window</code> and <code>stride</code> of the
<code>FeatureDescriptor</code> in this <code><a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a></code> (if <code>segment_end_idxs</code>
is also None). (Note that the <code>stride</code> argument of this method takes
precedence over the <code>stride</code> property of the <code>FeatureDescriptor</code>s).
By default None.</li>
</ul>
</dd>
<dt><strong><code>segment_end_idxs</code></strong> :&ensp;<code>Union[list, np.ndarray, pd.Series, pd.Index]</code>, optional</dt>
<dd>
<p>The end indices for the segmented windows. If None, the end indices will be
computed from the data using either:</p>
<ul>
<li>the <code>segment_start_idxs</code> + the <code>window</code> size property of the
<code>FeatureDescriptor</code> in this <code><a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a></code> (if <code>segment_start_idxs</code>
is not None)</li>
<li>strided-window rolling on the data using <code>window</code> and <code>stride</code> of the
<code>FeatureDescriptor</code> in this <code><a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a></code> (if <code>segment_start_idxs</code>
is also None). (Note that the <code>stride</code> argument of this method takes
precedence over the <code>stride</code> property of the <code>FeatureDescriptor</code>s).
By default None.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When passing both <code>segment_start_idxs</code> and <code>segment_end_idxs</code>, these two
arguments must have the same length and every start index must be &lt;=
than the corresponding end index.
Note that passing both arguments, discards any meaning of the <code>window</code>
and <code>stride</code> values (as these segment indices define the segmented data,
and thus no strided-window rolling index calculation has to be executed).
As such, the user can create variable-length segmented windows. However,
in such cases, the user should be weary that the feature functions are
invariant to these (potentially variable-length) windows.</p>
</div>
</dd>
<dt><strong><code>return_df</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether the output needs to be a DataFrame or a list thereof, by default
False. If <code>True</code> the output dataframes will be merged to a DataFrame with an
outer merge.</dd>
<dt><strong><code>window_idx</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>
<p>The window's index position which will be used as index for the
feature_window aggregation. Must be either of: <code>["begin", "middle", "end"]</code>.
by <strong>default "end"</strong>. All features in this collection will use the same
window_idx.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code>window_idx</code>="end" uses the window's end (= right open bound) as
output index. </p>
<p><code>window_idx</code>="begin" uses the window's start idx (= left closed bound)
as output index.</p>
</div>
</dd>
<dt><strong><code>include_final_window</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>
<p>Whether the final (possibly incomplete) window should be included in the
strided-window segmentation, by default False.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The remarks below apply when <code>include_final_window</code> is set to True.
The user should be aware that the last window <em>might</em> be incomplete,
i.e.;</p>
<ul>
<li>when equally sampled, the last window <em>might</em> be smaller than the
the other windows.</li>
<li>when not equally sampled, the last window <em>might</em> not include all the
data points (as the begin-time + window-size comes after the last data
point).</li>
</ul>
<p>Note, that when equally sampled, the last window <em>will</em> be a full window
when:</p>
<ul>
<li>the stride is the sampling rate of the data (or stride = 1 for
sample-based configurations).<br>
<strong>Remark</strong>: that when <code>include_final_window</code> is set to False, the last
window (which is a full) window will not be included!</li>
<li><em>(len * sampling_rate - window_size) % stride = 0</em>. Remark that the
above case is a base case of this.</li>
</ul>
</div>
</dd>
<dt><strong><code>bound_method</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>
<p>The start-end bound methodology which is used to generate the slice ranges
when <code>data</code> consists of multiple series / columns.
Must be either of: <code>["inner", "inner-outer", "outer"]</code>, by default "inner".</p>
<ul>
<li>if <code>inner</code>, the inner-bounds of the series are returned.</li>
<li>if <code>inner-outer</code>, the left-inner and right-outer bounds of the series
are returned.</li>
<li>if <code>outer</code>, the outer-bounds of the series are returned.</li>
</ul>
</dd>
<dt><strong><code>approve_sparsity</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Bool indicating whether the user acknowledges that there may be sparsity
(i.e., irregularly sampled data), by default False.
If False and sparsity is observed, a warning is raised.</dd>
<dt><strong><code>show_progress</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, the progress will be shown with a progressbar, by default False.</dd>
<dt><strong><code>logging_file_path</code></strong> :&ensp;<code>Union[str, Path]</code>, optional</dt>
<dd>The file path where the logged messages are stored. If <code>None</code>, then no
logging <code>FileHandler</code> will be used and the logging messages are only pushed
to stdout. Otherwise, a logging <code>FileHandler</code> will write the logged messages
to the given file path. See also the <code><a title="tsflex.features.logger" href="logger.html">.logger</a></code> module.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>
<p>The number of processes used for the feature calculation. If <code>None</code>, then
the number returned by <em>os.cpu_count()</em> is used, by default None. </p>
<p>If n_jobs is either 0 or 1, the code will be executed sequentially without
creating a process pool. This is very useful when debugging, as the stack
trace will be more comprehensible.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Multiprocessed execution is not supported on Windows. Even when,
<code>n_jobs</code> is set &gt; 1, the feature extraction will still be executed
sequentially.
Why do we not support multiprocessing on Windows; see this issue
<a href="https://github.com/predict-idlab/tsflex/issues/51">https://github.com/predict-idlab/tsflex/issues/51</a></p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>It takes on avg. <em>300ms</em> to schedule everything with
multiprocessing. So if your sequential feature extraction code runs
faster than ~1s, it might not be worth it to parallelize the process
(and thus better leave <code>n_jobs</code> to 0 or 1).</p>
</div>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[List[pd.DataFrame], pd.DataFrame]</code></dt>
<dd>The calculated features.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>KeyError</code></dt>
<dd>Raised when a required key is not found in <code>data</code>.</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li>The (column-)names of the series in <code>data</code> represent the <code>series_names</code>.</li>
<li>If a <code>logging_file_path</code> is provided, the execution (time) info can be
retrieved by calling <code>logger.get_feature_logs(logging_file_path)</code>.
Be aware that the <code>logging_file_path</code> gets cleared before the logger pushes
logged messages. Hence, one should use a separate logging file for each
constructed processing and feature instance with this library.</li>
</ul></div>
</dd>
<dt id="tsflex.features.feature_collection.FeatureCollection.serialize"><code class="name flex">
<span>def <span class="ident">serialize</span></span>(<span>self, file_path)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def serialize(self, file_path: Union[str, Path]):
    &#34;&#34;&#34;Serialize this FeatureCollection instance.

    Parameters
    ----------
    file_path : Union[str, Path]
        The path where the `FeatureCollection` will be serialized.

    Note
    -----
    As we use [Dill](https://github.com/uqfoundation/dill){:target=&#34;_blank&#34;} to
    serialize the files, we can **also serialize functions which are defined in
    the local scope, like lambdas.**

    &#34;&#34;&#34;
    with open(file_path, &#34;wb&#34;) as f:
        dill.dump(self, f, recurse=True)</code></pre>
</details>
<div class="desc"><p>Serialize this FeatureCollection instance.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>Union[str, Path]</code></dt>
<dd>The path where the <code><a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a></code> will be serialized.</dd>
</dl>
<h2 id="note">Note</h2>
<p>As we use <a href="https://github.com/uqfoundation/dill" target="_blank">Dill</a> to
serialize the files, we can <strong>also serialize functions which are defined in
the local scope, like lambdas.</strong></p></div>
</dd>
<dt id="tsflex.features.feature_collection.FeatureCollection.reduce"><code class="name flex">
<span>def <span class="ident">reduce</span></span>(<span>self, feat_cols_to_keep)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reduce(self, feat_cols_to_keep: List[str]) -&gt; FeatureCollection:
    &#34;&#34;&#34;Create a reduced FeatureCollection instance based on `feat_cols_to_keep`.

    For example, this is useful to optimize feature-extraction inference
    (for your selected features) after performing a feature-selection procedure.

    Parameters
    ----------
    feat_cols_to_keep: List[str]
        A subset of the feature collection instance its column names.
        This corresponds to the columns / names of the output from `calculate`
        method that you want to keep.

    Returns
    -------
    FeatureCollection
        A new FeatureCollection object, which only withholds the FeatureDescriptors
        which constitute the `feat_cols_to_keep` output.

    Note
    ----
    Some FeatureDescriptor objects may have multiple **output-names**.&lt;br&gt;
    Hence, if you only want to retain _a subset_ of that FeatureDescriptor its
    feature outputs, you will still get **all features** as the new
    FeatureCollection is constructed by applying a filter on de FeatureDescriptor
    list and we thus not alter these FeatureDescriptor objects themselves.

    &#34;&#34;&#34;
    # dict in which we store all the { output_col_name : (UUID, FeatureDescriptor) }
    # items of our current FeatureCollection object
    manual_window = False
    if any(c.endswith(&#34;w=manual&#34;) for c in feat_cols_to_keep):
        assert all(c.endswith(&#34;w=manual&#34;) for c in feat_cols_to_keep)
        # As the windows are created manual, the FeatureCollection cannot contain
        # multiple windows for the same output name - input_series combination
        self._check_no_multiple_windows()
        manual_window = True
    feat_col_fd_mapping: Dict[str, Tuple[str, FeatureDescriptor]] = {}
    for (s_names, window), fd_list in self._feature_desc_dict.items():
        window = &#34;manual&#34; if manual_window else self._ws_to_str(window)
        for fd in fd_list:
            # As a single FeatureDescriptor can have multiple output col names, we
            # create a unique identifier for each FeatureDescriptor (on which we
            # will apply set-like operations later on to only retain all the unique
            # FeatureDescriptors)
            uuid_str = str(uuid.uuid4())
            for output_name in fd.function.output_names:
                # Reconstruct the feature column name
                feat_col_name = StridedRolling.construct_output_index(
                    series_keys=s_names, feat_name=output_name, win_str=window
                )
                feat_col_fd_mapping[feat_col_name] = (uuid_str, fd)

    assert all(fc in feat_col_fd_mapping for fc in feat_cols_to_keep)

    # Collect (uuid, FeatureDescriptor) for the feat_cols_to_keep
    fd_subset: List[Tuple[str, FeatureDescriptor]] = [
        feat_col_fd_mapping[fc] for fc in feat_cols_to_keep
    ]

    # Reduce to unique feature descriptor objects (based on uuid) and create a new
    # FeatureCollection for their deepcopy&#39;s.
    seen_uuids = set()
    return FeatureCollection(
        feature_descriptors=[
            deepcopy(unique_fd)
            for unique_fd in {
                fd
                for (uuid_str, fd) in fd_subset
                if uuid_str not in seen_uuids and not seen_uuids.add(uuid_str)
            }
        ]
    )</code></pre>
</details>
<div class="desc"><p>Create a reduced FeatureCollection instance based on <code>feat_cols_to_keep</code>.</p>
<p>For example, this is useful to optimize feature-extraction inference
(for your selected features) after performing a feature-selection procedure.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>feat_cols_to_keep</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>A subset of the feature collection instance its column names.
This corresponds to the columns / names of the output from <code>calculate</code>
method that you want to keep.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a></code></dt>
<dd>A new FeatureCollection object, which only withholds the FeatureDescriptors
which constitute the <code>feat_cols_to_keep</code> output.</dd>
</dl>
<h2 id="note">Note</h2>
<p>Some FeatureDescriptor objects may have multiple <strong>output-names</strong>.<br>
Hence, if you only want to retain <em>a subset</em> of that FeatureDescriptor its
feature outputs, you will still get <strong>all features</strong> as the new
FeatureCollection is constructed by applying a filter on de FeatureDescriptor
list and we thus not alter these FeatureDescriptor objects themselves.</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</article>
<div class="sidebar_container">
<nav id="sidebar">
<div id="sidebar_content">
<header>
<div style="text-align: left; padding-top: 15px;">
<a class="homelink" rel="home" title="tsflex home" href="/tsflex/">
<img src="https://raw.githubusercontent.com/predict-idlab/tsflex/main/docs/_static/logo.png"
alt="logo should be displayed here" width="95%"></a>
</div>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tsflex.features" href="index.html">.features</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tsflex.features.feature_collection.FeatureCollection" href="#tsflex.features.feature_collection.FeatureCollection">FeatureCollection</a></code></h4>
<ul class="">
<li><code><a title="tsflex.features.feature_collection.FeatureCollection.get_required_series" href="#tsflex.features.feature_collection.FeatureCollection.get_required_series">get_required_series</a></code></li>
<li><code><a title="tsflex.features.feature_collection.FeatureCollection.get_nb_output_features" href="#tsflex.features.feature_collection.FeatureCollection.get_nb_output_features">get_nb_output_features</a></code></li>
<li><code><a title="tsflex.features.feature_collection.FeatureCollection.add" href="#tsflex.features.feature_collection.FeatureCollection.add">add</a></code></li>
<li><code><a title="tsflex.features.feature_collection.FeatureCollection.calculate" href="#tsflex.features.feature_collection.FeatureCollection.calculate">calculate</a></code></li>
<li><code><a title="tsflex.features.feature_collection.FeatureCollection.serialize" href="#tsflex.features.feature_collection.FeatureCollection.serialize">serialize</a></code></li>
<li><code><a title="tsflex.features.feature_collection.FeatureCollection.reduce" href="#tsflex.features.feature_collection.FeatureCollection.reduce">reduce</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</nav>
</div>
</main>
<script>
const sidebar = document.querySelector("body > main > div");
const sidebar_nav = document.querySelector("body > main > div > nav");
const sidebar_content = document.getElementById("sidebar_content");
document.getElementById("index_button_button").onclick = function () {
sidebar.classList.toggle('sidebar_small');
sidebar_nav.classList.toggle('hide_content');
sidebar_content.classList.toggle('hide_content');
}
</script>
</body>
</html>